{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":25046,"databundleVersionId":1852010,"sourceType":"competition"},{"sourceId":4597716,"sourceType":"datasetVersion","datasetId":2678471},{"sourceId":10216981,"sourceType":"datasetVersion","datasetId":6315439}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#the basics\nimport pandas as pd, numpy as np\nimport math, json, gc, random, os, sys\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#tensorflow deep learning basics\nimport tensorflow as tf\n#import tensorflow_addons as tfa\n\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Layer,Input,Activation, Lambda,Conv1D, SpatialDropout1D,Convolution1D,Dense,add,GlobalMaxPooling1D,GlobalAveragePooling1D,concatenate,Embedding\nfrom tensorflow.keras.models import Model\nfrom typing import List, Tuple\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split, KFold,  StratifiedKFold\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Conv1D, ReLU, BatchNormalization, Dropout\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, Add\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Ensure plots are rendered inline (if using a Jupyter notebook)\n%matplotlib inline\n\n# -------------------------------\n# 1. Load and Prepare the Dataset\n# -------------------------------\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\ndf = pd.read_csv(file_path)\n\n# Convert 'Timestamp' column to datetime for time-series analysis\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\n# List of numerical columns for later use\nnumerical_cols = ['Energy_Demand', 'Energy_Supply', 'Temperature', 'Grid_Load',\n                  'Renewable_Source_Output', 'NonRenewable_Source_Output', 'Energy_Price']\n\n# Set a common plot style\nsns.set(style=\"whitegrid\")\n\n# -------------------------------\n# 2. Time Series Analysis\n# -------------------------------\nplt.figure(figsize=(14, 6))\nplt.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand', color='blue')\nplt.plot(df['Timestamp'], df['Energy_Supply'], label='Energy Supply', color='green')\nplt.plot(df['Timestamp'], df['Temperature'], label='Temperature', color='red')\nplt.xlabel('Timestamp')\nplt.ylabel('Value')\nplt.title('Time Series of Energy Demand, Energy Supply, and Temperature')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# -------------------------------\n# 3. Distribution Analysis: Histograms & KDE Plots\n# -------------------------------\n# Histograms for numerical features\ndf[numerical_cols].hist(bins=15, figsize=(15, 10), layout=(3, 3))\nplt.suptitle(\"Histograms of Numerical Features\", fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n# KDE plots for numerical features (using fill=True instead of the deprecated shade=True)\nplt.figure(figsize=(14, 8))\nfor col in numerical_cols:\n    sns.kdeplot(data=df, x=col, fill=True, label=col)\nplt.title('KDE Plot of Numerical Features')\nplt.xlabel('Value')\nplt.legend()\nplt.show()\n\n# -------------------------------\n# 4. Correlation Analysis: Heatmap and Pairplot\n# -------------------------------\n# Compute correlation matrix for numerical columns\ncorr = df[numerical_cols].corr()\n\n# Correlation Heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numerical Features\")\nplt.show()\n\n# Pairplot for pairwise relationships (this may take a moment)\nsns.pairplot(df[numerical_cols])\nplt.suptitle(\"Pairwise Relationships Between Numerical Features\", y=1.02)\nplt.show()\n\n# -------------------------------\n# 5. Categorical Analysis: Weather Conditions\n# -------------------------------\n# Count plot for Weather_Condition_x\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Weather_Condition_x', data=df, palette='Set2')\nplt.title('Count of Observations by Weather_Condition_x')\nplt.xlabel('Weather Condition (x)')\nplt.ylabel('Count')\nplt.show()\n\n# Count plot for Weather_Condition_y\nplt.figure(figsize=(8, 4))\nsns.countplot(x='Weather_Condition_y', data=df, palette='Set3')\nplt.title('Count of Observations by Weather_Condition_y')\nplt.xlabel('Weather Condition (y)')\nplt.ylabel('Count')\nplt.show()\n\n# Box plots to see how weather conditions affect energy demand and supply (using Weather_Condition_x)\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Weather_Condition_x', y='Energy_Demand', data=df, palette='pastel')\nplt.title('Energy Demand by Weather_Condition_x')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\nsns.boxplot(x='Weather_Condition_x', y='Energy_Supply', data=df, palette='pastel')\nplt.title('Energy Supply by Weather_Condition_x')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------\n# 6. Scatter Plots: Exploring Relationships\n# -------------------------------\n# Scatter plot: Grid Load vs. Energy Demand, colored by Weather_Condition_x\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Grid_Load', y='Energy_Demand', data=df, hue='Weather_Condition_x', palette='deep')\nplt.title('Grid Load vs. Energy Demand')\nplt.xlabel('Grid Load')\nplt.ylabel('Energy Demand')\nplt.legend(title='Weather Condition')\nplt.show()\n\n# Scatter plot: Renewable vs NonRenewable Source Output, colored by Energy Price.\n# Since sns.scatterplot does not return a mappable for a colorbar, we create one manually.\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    x='Renewable_Source_Output',\n    y='NonRenewable_Source_Output',\n    data=df,\n    hue='Energy_Price',\n    palette='viridis',\n    legend=False  # We'll add our own colorbar\n)\nplt.title('Renewable vs NonRenewable Source Output Colored by Energy Price')\nplt.xlabel('Renewable Source Output')\nplt.ylabel('NonRenewable Source Output')\n\n# Create a normalization and ScalarMappable for the colorbar\nnorm = plt.Normalize(df['Energy_Price'].min(), df['Energy_Price'].max())\nsm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\nsm.set_array([])  # Required for older versions of matplotlib\ncbar = plt.colorbar(sm, label='Energy Price')\nplt.show()\n\n# -------------------------------\n# 7. Combined Time Series Plot with Dual Y-axis\n# -------------------------------\nfig, ax1 = plt.subplots(figsize=(14, 6))\n\n# Plot energy metrics on the primary y-axis\nax1.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand', color='blue')\nax1.plot(df['Timestamp'], df['Energy_Supply'], label='Energy Supply', color='green')\nax1.plot(df['Timestamp'], df['Temperature'], label='Temperature', color='red')\nax1.set_xlabel('Timestamp')\nax1.set_ylabel('Energy/Temperature Values')\nax1.tick_params(axis='x', rotation=45)\nax1.legend(loc='upper left')\n\n# Plot Energy Price on a secondary y-axis\nax2 = ax1.twinx()\nax2.plot(df['Timestamp'], df['Energy_Price'], label='Energy Price', color='purple', linestyle='--')\nax2.set_ylabel('Energy Price')\nax2.legend(loc='upper right')\n\nplt.title(\"Combined Time Series Plot: Energy Metrics and Price\")\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:20:00.416938Z","iopub.execute_input":"2025-04-13T13:20:00.417496Z","iopub.status.idle":"2025-04-13T13:20:32.250043Z","shell.execute_reply.started":"2025-04-13T13:20:00.417450Z","shell.execute_reply":"2025-04-13T13:20:32.248949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to your dataset\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Print information about the DataFrame (such as column names, non-null counts, and data types)\nprint(\"Data Information:\")\nprint(df.info())\n\n# Print the first 5 rows of the DataFrame\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:20:32.251252Z","iopub.execute_input":"2025-04-13T13:20:32.251571Z","iopub.status.idle":"2025-04-13T13:20:32.280665Z","shell.execute_reply.started":"2025-04-13T13:20:32.251532Z","shell.execute_reply":"2025-04-13T13:20:32.279800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check unique values in both columns\nprint(df[['Weather_Condition_x', 'Weather_Condition_y']].drop_duplicates())\n\n# Compare if they are the same\nprint((df['Weather_Condition_x'] == df['Weather_Condition_y']).value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:20:32.282082Z","iopub.execute_input":"2025-04-13T13:20:32.282450Z","iopub.status.idle":"2025-04-13T13:20:32.295139Z","shell.execute_reply.started":"2025-04-13T13:20:32.282389Z","shell.execute_reply":"2025-04-13T13:20:32.294395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to your dataset\nfile_path = \"/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv\"\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Print basic DataFrame information\nprint(\"Data Information:\")\ndf.info()  # This prints the info to stdout\n\n# Print the first 5 rows of the DataFrame\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(df.head())\n\n# Convert the Timestamp column to datetime format for easier time-based operations\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\nprint(\"\\nData Types After Converting Timestamp:\")\nprint(df.dtypes)\n\n# Print summary statistics for numeric columns\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n\n# If you want to inspect the weather condition columns, you can print unique values\nprint(\"\\nUnique values in 'Weather_Condition_x':\")\nprint(df['Weather_Condition_x'].unique())\n\nprint(\"\\nUnique values in 'Weather_Condition_y':\")\nprint(df['Weather_Condition_y'].unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:20:32.296504Z","iopub.execute_input":"2025-04-13T13:20:32.296835Z","iopub.status.idle":"2025-04-13T13:20:32.341369Z","shell.execute_reply.started":"2025-04-13T13:20:32.296804Z","shell.execute_reply":"2025-04-13T13:20:32.340471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Energy Demand over Time\nplt.figure(figsize=(10, 6))\nplt.plot(df['Timestamp'], df['Energy_Demand'], label='Energy Demand')\nplt.xlabel('Time')\nplt.ylabel('Energy Demand')\nplt.title('Energy Demand Over Time')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:20:32.342289Z","iopub.execute_input":"2025-04-13T13:20:32.342666Z","iopub.status.idle":"2025-04-13T13:20:32.730188Z","shell.execute_reply.started":"2025-04-13T13:20:32.342633Z","shell.execute_reply":"2025-04-13T13:20:32.729359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. iTBG-Net: A Hybrid Deep Learning Model Combining Temporal Convolutional Networks and BiGRU","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport random\nfrom collections import deque\nfrom tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, Add, Dropout\n\n\n#  Data Loading and Splitting into Training and Forecasting\n\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\n\nfeatures = ['Energy_Demand']\nsequence_length = 48  \nprediction_length = 48  \n\nn_total = len(data)\nprint (\"The Total samples are in the dataset:\", len(data))\nn_forecast = prediction_length  # Reserving data for last 48 hours data for forecasting\nn_train = n_total - n_forecast\ntraining_data = data.iloc[:n_train].copy()\nforecast_data = data.iloc[n_train:].copy()\nprint(\"The Samples for the Final forecasting are: \" ,(n_forecast))\nprint (\"The total Number of the train samples are:\" ,(n_train))\n\n\n\n#  Mean-Based Scaling\n\ntraining_mean = training_data[features].mean().values[0]\ntraining_features = training_data[features].values / training_mean\nforecast_features = forecast_data[features].values / training_mean\n\n\n#  Generate  Sequences from Training Data (Sliding Windows)\n\ntraining_sequences, training_labels = [], []\nfor i in range(len(training_features) - sequence_length - prediction_length + 1):\n    seq = training_features[i : i + sequence_length]\n    label = training_features[i + sequence_length : i + sequence_length + prediction_length]\n    training_sequences.append(seq)\n    training_labels.append(label)\n\ntraining_sequences = np.array(training_sequences)\ntraining_labels = np.array(training_labels, dtype=np.float32)\n\n# Define forecast sequence using the last sequence_length hours from training_features.\nforecast_sequence = training_features[-sequence_length:]\nforecast_label = forecast_features\n\n\n\n#  Replay Buffer for Incremental Learning\n\nclass ReplayBuffer:\n    def __init__(self, max_size=5000):\n        self.buffer = deque(maxlen=max_size)\n        \n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n        \n    def sample(self, batch_size):\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        else:\n            return random.sample(self.buffer, batch_size)\n\n\n#  Define TCN Block and Build the Hybrid Model \n\n\ndef tcn_block(filters, kernel_size, dilation_rate, dropout):\n    def block(x):\n        # Residual connection to match dimensions.\n        res = Conv1D(filters, kernel_size=1, padding=\"same\")(x)\n        conv1 = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding=\"causal\", activation=None)(x)\n        norm1 = BatchNormalization()(conv1)\n        act1 = ReLU()(norm1)\n        drop1 = Dropout(dropout)(act1)\n        conv2 = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding=\"causal\", activation=None)(drop1)\n        norm2 = BatchNormalization()(conv2)\n        act2 = ReLU()(norm2)\n        drop2 = Dropout(dropout)(act2)\n        skip = Add()([res, drop2])\n        return skip\n    return block\n\ndef build_hybrid_model(seq_len, pred_len, dropout=0.1, hidden_dim=64):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n    tcn_output = tcn_block(hidden_dim, kernel_size=1, dilation_rate=1, dropout=dropout)(inputs)\n    tcn_output = tcn_block(hidden_dim, kernel_size=1, dilation_rate=1, dropout=dropout)(tcn_output)\n    \n    gru_output = tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )(tcn_output)\n    \n    lstm_output = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )(gru_output)\n    \n    \n    truncated = lstm_output[:, :pred_len, :]\n    outputs = tf.keras.layers.Dense(1, activation='linear')(truncated)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n        loss='mse',\n        metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n    )\n    return model\n\n# Build and visualize the new hybrid model \n\nhybrid_model = build_hybrid_model(seq_len=sequence_length, pred_len=prediction_length, dropout=0.1, hidden_dim=64)\nhybrid_model.summary()\ntf.keras.utils.plot_model(\n    hybrid_model,\n    to_file=\"hybrid_model_tcn_gru_lstm_architecture.png\",\n    show_shapes=True,\n    show_layer_names=True\n)\n\n\n# Incremental Online Training Loop with Validation (Update-level Metrics)\n\nclass RegressionAccuracyCallback(tf.keras.callbacks.Callback):\n    def __init__(self, train_data, val_data, tol=0.1):\n        super().__init__()\n        self.train_data = train_data\n        self.val_data = val_data\n        self.tol = tol\n        self.train_accuracies = []\n        self.val_accuracies = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        train_preds = self.model.predict(self.train_data[0], verbose=0)\n        train_acc = np.mean(np.abs(train_preds - self.train_data[1]) < self.tol)\n        self.train_accuracies.append(train_acc)\n        \n        val_preds = self.model.predict(self.val_data[0], verbose=0)\n        val_acc = np.mean(np.abs(val_preds - self.val_data[1]) < self.tol)\n        self.val_accuracies.append(val_acc)\n\nbatch_size_update = 16      \nepochs_per_update = 80      \nreplay_sample_size = 16\n\n# We'll record one set of metrics per incremental update\n\nmetrics_history = {\n    \"incremental_update\": [],\n    \"avg_loss\": [],\n    \"avg_val_loss\": [],\n    \"avg_accuracy\": [],\n    \"avg_val_accuracy\": [],\n    \"update_time\": []\n}\n\nreplay_buffer = ReplayBuffer(max_size=5000)\n\nnum_training_updates = int(np.ceil(len(training_sequences) / batch_size_update))\nprint(f\"Starting incremental training over {num_training_updates} updates...\\n\")\n\nfor update_idx in range(0, len(training_sequences), batch_size_update):\n    # Get new training samples for the current update\n    new_sequences = training_sequences[update_idx:update_idx + batch_size_update]\n    new_labels = training_labels[update_idx:update_idx + batch_size_update]\n    update_length = len(new_sequences)\n    \n    # Determine time boundaries for logging (based on training_data timestamps)\n    \n    update_input_start = training_data['Timestamp'].iloc[update_idx]\n    update_input_end = training_data['Timestamp'].iloc[min(update_idx + update_length + sequence_length - 1, len(training_data)-1)]\n    update_label_start = training_data['Timestamp'].iloc[update_idx + sequence_length]\n    update_label_end = training_data['Timestamp'].iloc[min(update_idx + update_length + sequence_length + prediction_length - 1, len(training_data)-1)]\n    \n    current_update = update_idx // batch_size_update + 1\n    print(f\"Processing Incremental update {current_update}/{num_training_updates}:\")\n    print(f\"  - Training Input Hours: from {update_input_start} to {update_input_end}\")\n    print(f\"  - Training Label Hours: from {update_label_start} to {update_label_end}\")\n    \n    # Add new samples to the replay buffer\n    \n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    # Sample from the replay buffer to mix with new data\n    replay_samples = replay_buffer.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new and replay data\n    \n    combined_sequences = np.vstack((new_sequences, replay_sequences))\n    combined_labels = np.vstack((new_labels, replay_labels))\n    \n    # Split combined data into training and validation sets (80/20 split)\n    \n    total_combined = combined_sequences.shape[0]\n    num_val = int(np.ceil(total_combined * 0.2))\n    num_train = total_combined - num_val\n    train_X = combined_sequences[:num_train]\n    train_y = combined_labels[:num_train]\n    val_X = combined_sequences[num_train:]\n    val_y = combined_labels[num_train:]\n    \n    # Create a callback to compute regression accuracy per epoch\n    \n    acc_callback = RegressionAccuracyCallback(train_data=(train_X, train_y), val_data=(val_X, val_y), tol=0.1)\n    \n    start_time_update = time.time()\n    history = hybrid_model.fit(\n        train_X, train_y,\n        batch_size=batch_size_update,\n        epochs=epochs_per_update,\n        verbose=0,\n        validation_data=(val_X, val_y),\n        shuffle=False,\n        callbacks=[acc_callback]\n    )\n    end_time_update = time.time()\n    \n    update_time = end_time_update - start_time_update\n    avg_loss = np.mean(history.history['loss'])\n    avg_val_loss = np.mean(history.history['val_loss'])\n    avg_accuracy = np.mean(acc_callback.train_accuracies)\n    avg_val_accuracy = np.mean(acc_callback.val_accuracies)\n    \n    # Record update-level metrics\n    metrics_history[\"incremental_update\"].append(current_update)\n    metrics_history[\"avg_loss\"].append(avg_loss)\n    metrics_history[\"avg_val_loss\"].append(avg_val_loss)\n    metrics_history[\"avg_accuracy\"].append(avg_accuracy)\n    metrics_history[\"avg_val_accuracy\"].append(avg_val_accuracy)\n    metrics_history[\"update_time\"].append(update_time)\n    \n    print(f\"Incremental update {current_update}/{num_training_updates}: \"\n          f\"Avg Loss = {avg_loss:.4f}, \"\n          f\"Avg Val Loss = {avg_val_loss:.4f}, \"\n          f\"Update Time = {update_time:.2f}s\\n\")\n\n# Save the trained model\nmodel_save_path = \"hybrid_model.h5\"\nhybrid_model.save(model_save_path)\nprint(f\"Model saved to {model_save_path}\\n\")\n\n\n# Forecasting and Plotting\n\n\npredicted_values_ratio = hybrid_model.predict(np.expand_dims(forecast_sequence, axis=0)).flatten()\nactual_values_ratio = forecast_label.flatten()\n\n# Convert back to actual units\npredicted_values = predicted_values_ratio * training_mean\nactual_values = actual_values_ratio * training_mean\n\n# Discard fractional parts by converting to integers.\n\npredicted_values_int = predicted_values.astype(int)\nactual_values_int = actual_values.astype(int)\n\n# Print forecasted values along with the actual ground truth.\n\nprint(\"Forecasted Values:\")\nprint(predicted_values_int)\nprint(\"Actual Ground Truth:\")\nprint(actual_values_int)\n\nfig, ax = plt.subplots(figsize=(6, 4), dpi=300)\nax.plot(range(prediction_length), actual_values_int, label=\"Actual\", marker=\"*\", linewidth=1, color=\"firebrick\")\nax.plot(range(prediction_length), predicted_values_int, label=\"Forecasted (iTBG-Net)\", linestyle=\"--\", marker=\".\", linewidth=1, color=\"blue\")\nax.set_title(\"Actual vs Predicted Energy Demand (kWh)\", fontsize=12, fontweight=\"bold\")\nax.set_ylabel(\"Energy Consumption (kWh)\", fontsize=10, fontweight=\"bold\")\nax.legend(loc=\"upper right\", fontsize=10)\nax.grid(True, linestyle=\"--\", alpha=0.8)\nax.set_ylim(600, 1400)\nplt.tight_layout()\nplt.savefig(\"Final_Forecasting.eps\", dpi=300, bbox_inches=\"tight\")\nplt.savefig(\"Final_Forecasting.pdf\", dpi=300, bbox_inches=\"tight\")\nplt.savefig(\"Final_Forecasting.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n#  Plot Training Loss per Incremental Update\n\n\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_loss\"], label=\"Train Loss\", marker=\"*\", linewidth=1, color=\"crimson\")\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_val_loss\"], label=\"Val Loss\", marker=\".\",linewidth=1, color=\"royalblue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Loss\", fontsize=10, fontweight=\"bold\")\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"PM_loss_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n# Plot Regression Accuracy per Incremental Update\n\n\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_accuracy\"], label=\"Train Accuracy\",  marker=\"*\", linewidth=1, color=\"crimson\")\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_val_accuracy\"], label=\"Val Accuracy\", marker=\".\", linewidth=1, color=\"royalblue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Regression Accuracy\", fontsize=10, fontweight=\"bold\")\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"PM_regression_accuracy_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n# Plot Training Time per Incremental Update \n\n\nupdates_arr = np.array(metrics_history[\"incremental_update\"])\nupdate_times = np.array(metrics_history[\"update_time\"])\nselected_indices = np.arange(0, len(updates_arr), 2)  # adjust the step as needed\nselected_updates = updates_arr[selected_indices]\nselected_update_times = update_times[selected_indices]\n\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(selected_updates, selected_update_times, marker=\"o\", linestyle=\"-\", markersize=5, linewidth=2,\n         color=\"blue\", markerfacecolor=\"white\", markeredgewidth=1.2, markeredgecolor=\"blue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Training Time (seconds)\", fontsize=10, fontweight=\"bold\")\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"PM_Training_Time_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n# Save the Performance Metrics for Later Comparison\n\n\nnp.savez(\"hybrid_metrics.npz\",\n         incremental_update=np.array(metrics_history[\"incremental_update\"]),\n         avg_loss=np.array(metrics_history[\"avg_loss\"]),\n         avg_val_loss=np.array(metrics_history[\"avg_val_loss\"]),\n         avg_accuracy=np.array(metrics_history[\"avg_accuracy\"]),\n         avg_val_accuracy=np.array(metrics_history[\"avg_val_accuracy\"]),\n         update_time=np.array(metrics_history[\"update_time\"]),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"Hybrid model metrics saved to hybrid_metrics.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:20:32.731037Z","iopub.execute_input":"2025-04-13T13:20:32.731274Z","iopub.status.idle":"2025-04-13T13:26:53.119662Z","shell.execute_reply.started":"2025-04-13T13:20:32.731253Z","shell.execute_reply":"2025-04-13T13:26:53.118960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. CNN Model Incremental Learning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport random\nfrom collections import deque\nfrom tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout\n# Other necessary imports remain the same.\n\n# ===========================\n# 1. Data Loading and Splitting into Training and Forecasting\n# ===========================\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\n# Use only the target column \"Energy_Demand\"\nfeatures = ['Energy_Demand']\nsequence_length = 48  \nprediction_length = 48  \n\nn_total = len(data)\nn_forecast = prediction_length  # Reserve the last 48 hours for forecasting\nn_train = n_total - n_forecast\n\ntraining_data = data.iloc[:n_train].copy()\nforecast_data = data.iloc[n_train:].copy()\n\n# ===========================\n# 2. Mean-Based Scaling\n# ===========================\ntraining_mean = training_data[features].mean().values[0]\ntraining_features = training_data[features].values / training_mean\nforecast_features = forecast_data[features].values / training_mean\n\n# ===========================\n# 3. Generate Overlapping Sequences from Training Data\n# ===========================\ntraining_sequences, training_labels = [], []\nfor i in range(len(training_features) - sequence_length - prediction_length + 1):\n    seq = training_features[i : i + sequence_length]\n    label = training_features[i + sequence_length : i + sequence_length + prediction_length]\n    training_sequences.append(seq)\n    training_labels.append(label)\n\ntraining_sequences = np.array(training_sequences)\ntraining_labels = np.array(training_labels, dtype=np.float32)\n\n# Define forecast sequence using the last sequence_length hours from training_features.\nforecast_sequence = training_features[-sequence_length:]\nforecast_label = forecast_features\n\n# ===========================\n# 4. Replay Buffer for Incremental Learning\n# ===========================\nclass ReplayBuffer:\n    def __init__(self, max_size=5000):\n        self.buffer = deque(maxlen=max_size)\n        \n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n        \n    def sample(self, batch_size):\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        else:\n            return random.sample(self.buffer, batch_size)\n\n# ===========================\n# 5. Define the Complex CNN Model (Nearly 191K Parameters)\n# ===========================\ndef build_cnn_model(seq_len=sequence_length, pred_len=prediction_length,\n                    dropout=0.4, hidden_dim=128, dense_units=318):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n    \n    # --- Convolutional Block 1 ---\n    x = Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Dropout(dropout)(x)\n    \n    # --- Convolutional Block 2 ---\n    x = Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dropout)(x)\n    \n    # --- Convolutional Block 3 ---\n    x = Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dropout)(x)\n    \n    # --- Convolutional Block 4 ---\n    x = Conv1D(filters=hidden_dim, kernel_size=3, activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dropout)(x)\n    \n    # --- TimeDistributed Dense Layers ---\n    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(dense_units, activation='relu'))(x)\n    x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(x)\n    \n    # Truncate the output to ensure it matches the prediction length.\n    outputs = tf.keras.layers.Lambda(lambda t: t[:, :pred_len, :])(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n    )\n    return model\n\ncnn_model = build_cnn_model()\ncnn_model.summary()\ntf.keras.utils.plot_model(\n    cnn_model,\n    to_file=\"complex_cnn_model_architecture.png\",\n    show_shapes=True,\n    show_layer_names=True\n)\n\n# ===========================\n# 6. Incremental Online Training Loop with Validation (Update-level Metrics)\n# ===========================\nclass RegressionAccuracyCallback(tf.keras.callbacks.Callback):\n    def __init__(self, train_data, val_data, tol=0.1):\n        super().__init__()\n        self.train_data = train_data\n        self.val_data = val_data\n        self.tol = tol\n        self.train_accuracies = []\n        self.val_accuracies = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        train_preds = self.model.predict(self.train_data[0], verbose=0)\n        train_acc = np.mean(np.abs(train_preds - self.train_data[1]) < self.tol)\n        self.train_accuracies.append(train_acc)\n        \n        val_preds = self.model.predict(self.val_data[0], verbose=0)\n        val_acc = np.mean(np.abs(val_preds - self.val_data[1]) < self.tol)\n        self.val_accuracies.append(val_acc)\n\nbatch_size_update = 16      \nepochs_per_update = 80      \nreplay_sample_size = 16\n\n# We'll record one set of metrics per incremental update\nmetrics_history = {\n    \"incremental_update\": [],\n    \"avg_loss\": [],\n    \"avg_val_loss\": [],\n    \"avg_accuracy\": [],\n    \"avg_val_accuracy\": [],\n    \"update_time\": []\n}\n\nreplay_buffer = ReplayBuffer(max_size=5000)\n\nnum_training_updates = int(np.ceil(len(training_sequences) / batch_size_update))\nprint(f\"Starting incremental training over {num_training_updates} updates...\\n\")\n\nfor update_idx in range(0, len(training_sequences), batch_size_update):\n    # Get new training samples for the current update\n    new_sequences = training_sequences[update_idx:update_idx + batch_size_update]\n    new_labels = training_labels[update_idx:update_idx + batch_size_update]\n    update_length = len(new_sequences)\n    \n    # Determine time boundaries for logging (based on training_data timestamps)\n    update_input_start = training_data['Timestamp'].iloc[update_idx]\n    update_input_end = training_data['Timestamp'].iloc[min(update_idx + update_length + sequence_length - 1, len(training_data)-1)]\n    update_label_start = training_data['Timestamp'].iloc[update_idx + sequence_length]\n    update_label_end = training_data['Timestamp'].iloc[min(update_idx + update_length + sequence_length + prediction_length - 1, len(training_data)-1)]\n    \n    current_update = update_idx // batch_size_update + 1\n    print(f\"Processing Incremental update {current_update}/{num_training_updates}:\")\n    print(f\"  - Training Input Hours: from {update_input_start} to {update_input_end}\")\n    print(f\"  - Training Label Hours: from {update_label_start} to {update_label_end}\")\n    \n    # Add new samples to the replay buffer\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    # Sample from the replay buffer to mix with new data\n    replay_samples = replay_buffer.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new and replay data\n    combined_sequences = np.vstack((new_sequences, replay_sequences))\n    combined_labels = np.vstack((new_labels, replay_labels))\n    \n    # Split combined data into training and validation sets (80/20 split)\n    total_combined = combined_sequences.shape[0]\n    num_val = int(np.ceil(total_combined * 0.2))\n    num_train = total_combined - num_val\n    train_X = combined_sequences[:num_train]\n    train_y = combined_labels[:num_train]\n    val_X = combined_sequences[num_train:]\n    val_y = combined_labels[num_train:]\n    \n    # Create a callback to compute regression accuracy per epoch\n    acc_callback = RegressionAccuracyCallback(train_data=(train_X, train_y), val_data=(val_X, val_y), tol=0.1)\n    \n    start_time_update = time.time()\n    history = cnn_model.fit(\n        train_X, train_y,\n        batch_size=batch_size_update,\n        epochs=epochs_per_update,\n        verbose=0,\n        validation_data=(val_X, val_y),\n        shuffle=False,\n        callbacks=[acc_callback]\n    )\n    end_time_update = time.time()\n    \n    update_time = end_time_update - start_time_update\n    avg_loss = np.mean(history.history['loss'])\n    avg_val_loss = np.mean(history.history['val_loss'])\n    avg_accuracy = np.mean(acc_callback.train_accuracies)\n    avg_val_accuracy = np.mean(acc_callback.val_accuracies)\n    \n    # Record update-level metrics\n    metrics_history[\"incremental_update\"].append(current_update)\n    metrics_history[\"avg_loss\"].append(avg_loss)\n    metrics_history[\"avg_val_loss\"].append(avg_val_loss)\n    metrics_history[\"avg_accuracy\"].append(avg_accuracy)\n    metrics_history[\"avg_val_accuracy\"].append(avg_val_accuracy)\n    metrics_history[\"update_time\"].append(update_time)\n    \n    print(f\"Incremental update {current_update}/{num_training_updates}: \"\n          f\"Avg Loss = {avg_loss:.4f}, \"\n          f\"Avg Val Loss = {avg_val_loss:.4f}, \"\n          f\"Update Time = {update_time:.2f}s\\n\")\n\n# Save the trained model\nmodel_save_path = \"cnn_model.h5\"\ncnn_model.save(model_save_path)\nprint(f\"Model saved to {model_save_path}\\n\")\n\n# ===========================\n# 7. Forecasting and Plotting\n# ===========================\npredicted_values_ratio = cnn_model.predict(np.expand_dims(forecast_sequence, axis=0)).flatten()\nactual_values_ratio = forecast_label.flatten()\n\n# Convert back to actual units\npredicted_values = predicted_values_ratio * training_mean\nactual_values = actual_values_ratio * training_mean\n\n# Discard fractional parts by converting to integers.\npredicted_values_int = predicted_values.astype(int)\nactual_values_int = actual_values.astype(int)\n\n# Print forecasted values along with the actual ground truth.\nprint(\"Forecasted Values:\")\nprint(predicted_values_int)\nprint(\"Actual Ground Truth:\")\nprint(actual_values_int)\n\nfig, ax = plt.subplots(figsize=(6, 4), dpi=300)\nax.plot(range(prediction_length), actual_values_int, label=\"Actual\", marker=\"*\", linewidth=1, color=\"firebrick\")\nax.plot(range(prediction_length), predicted_values_int, label=\"Forecasted (CNN-IL)\", linestyle=\"--\", marker=\".\", linewidth=1, color=\"blue\")\nax.set_title(\"Actual vs Predicted Energy Demand (kWh)\", fontsize=12, fontweight=\"bold\")\nax.set_ylabel(\"Energy Consumption (kWh)\", fontsize=10, fontweight=\"bold\")\nax.legend(loc=\"upper right\", fontsize=10)\nax.grid(True, linestyle=\"--\", alpha=0.8)\nax.set_ylim(600, 1400)\nplt.tight_layout()\nplt.savefig(\"Final_Forecasting_CNN.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 8. Plot Training Loss per Incremental Update\n# ===========================\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_loss\"], label=\"Train Loss\", marker=\"*\", linewidth=1, color=\"crimson\")\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_val_loss\"], label=\"Val Loss\", marker=\".\", linewidth=1, color=\"royalblue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Loss\", fontsize=10, fontweight=\"bold\")\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"CNN_PM_loss_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 9. Plot Regression Accuracy per Incremental Update\n# ===========================\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_accuracy\"], label=\"Train Accuracy\",  marker=\"*\", linewidth=1, color=\"crimson\")\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_val_accuracy\"], label=\"Val Accuracy\", marker=\".\", linewidth=1, color=\"royalblue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Regression Accuracy\", fontsize=10, fontweight=\"bold\")\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"CNN_PM_regression_accuracy_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 10. Plot Training Time per Incremental Update (Selected Updates)\n# ===========================\nupdates_arr = np.array(metrics_history[\"incremental_update\"])\nupdate_times = np.array(metrics_history[\"update_time\"])\nselected_indices = np.arange(0, len(updates_arr), 2)  # adjust the step as needed\nselected_updates = updates_arr[selected_indices]\nselected_update_times = update_times[selected_indices]\n\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(selected_updates, selected_update_times, marker=\"o\", linestyle=\"-\", markersize=5, linewidth=2,\n         color=\"blue\", markerfacecolor=\"white\", markeredgewidth=1.2, markeredgecolor=\"blue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Training Time (seconds)\", fontsize=10, fontweight=\"bold\")\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"CNN_PM_Training_Time_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 11. Save the Performance Metrics for Later Comparison\n# ===========================\nnp.savez(\"cnn_metrics.npz\",\n         incremental_update=np.array(metrics_history[\"incremental_update\"]),\n         avg_loss=np.array(metrics_history[\"avg_loss\"]),\n         avg_val_loss=np.array(metrics_history[\"avg_val_loss\"]),\n         avg_accuracy=np.array(metrics_history[\"avg_accuracy\"]),\n         avg_val_accuracy=np.array(metrics_history[\"avg_val_accuracy\"]),\n         update_time=np.array(metrics_history[\"update_time\"]),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"CNN model metrics saved to cnn_metrics.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:26:53.120583Z","iopub.execute_input":"2025-04-13T13:26:53.121347Z","iopub.status.idle":"2025-04-13T13:32:30.649811Z","shell.execute_reply.started":"2025-04-13T13:26:53.121319Z","shell.execute_reply":"2025-04-13T13:32:30.649021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. LSTM Modeling Incremental Learning","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport random\nfrom collections import deque\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n\n# ===========================\n# 1. Data Loading and Splitting into Training and Forecasting\n# ===========================\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\n# Use only the target column \"Energy_Demand\"\nfeatures = ['Energy_Demand']\nsequence_length = 48  \nprediction_length = 48  \n\nn_total = len(data)\nn_forecast = prediction_length  # Reserve the last 48 hours for forecasting\nn_train = n_total - n_forecast\n\ntraining_data = data.iloc[:n_train].copy()\nforecast_data = data.iloc[n_train:].copy()\n\n# ===========================\n# 2. Mean-Based Scaling\n# ===========================\ntraining_mean = training_data[features].mean().values[0]\ntraining_features = training_data[features].values / training_mean\nforecast_features = forecast_data[features].values / training_mean\n\n# ===========================\n# 3. Generate Overlapping Sequences from Training Data\n# ===========================\ntraining_sequences, training_labels = [], []\nfor i in range(len(training_features) - sequence_length - prediction_length + 1):\n    seq = training_features[i : i + sequence_length]\n    label = training_features[i + sequence_length : i + sequence_length + prediction_length]\n    training_sequences.append(seq)\n    training_labels.append(label)\n\ntraining_sequences = np.array(training_sequences)\ntraining_labels = np.array(training_labels, dtype=np.float32)\n\n# Define forecast sequence using the last sequence_length hours from training_features.\nforecast_sequence = training_features[-sequence_length:]\nforecast_label = forecast_features\n\n# ===========================\n# 4. Replay Buffer for Incremental Learning\n# ===========================\nclass ReplayBuffer:\n    def __init__(self, max_size=5000):\n        self.buffer = deque(maxlen=max_size)\n        \n    def add(self, sequence, label):\n        self.buffer.append((sequence, label))\n        \n    def sample(self, batch_size):\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        else:\n            return random.sample(self.buffer, batch_size)\n\n# ===========================\n# 5. Define the Stacked LSTM Model (Nearly 191K Parameters)\n# ===========================\ndef build_lstm_model(seq_len=sequence_length, pred_len=prediction_length, dropout=0.4):\n    inputs = tf.keras.layers.Input(shape=(seq_len, len(features)))\n    # First LSTM layer with 128 units.\n    x = LSTM(128, return_sequences=True, dropout=dropout)(inputs)\n    # Second LSTM layer with 123 units.\n    x = LSTM(123, return_sequences=True, dropout=dropout)(x)\n    # Map each timestep to the output value.\n    x = TimeDistributed(Dense(1, activation='linear'))(x)\n    # Truncate the output to ensure it matches the prediction length.\n    outputs = Lambda(lambda t: t[:, :pred_len, :])(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae', 'mse', tf.keras.metrics.MeanAbsolutePercentageError(name='mape')]\n    )\n    return model\n\nlstm_model = build_lstm_model()\nlstm_model.summary()\ntf.keras.utils.plot_model(\n    lstm_model,\n    to_file=\"lstm_model_architecture.png\",\n    show_shapes=True,\n    show_layer_names=True\n)\n\n# ===========================\n# 6. Incremental Online Training Loop with Validation (Update-level Metrics)\n# ===========================\nclass RegressionAccuracyCallback(tf.keras.callbacks.Callback):\n    def __init__(self, train_data, val_data, tol=0.1):\n        super().__init__()\n        self.train_data = train_data\n        self.val_data = val_data\n        self.tol = tol\n        self.train_accuracies = []\n        self.val_accuracies = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        train_preds = self.model.predict(self.train_data[0], verbose=0)\n        train_acc = np.mean(np.abs(train_preds - self.train_data[1]) < self.tol)\n        self.train_accuracies.append(train_acc)\n        \n        val_preds = self.model.predict(self.val_data[0], verbose=0)\n        val_acc = np.mean(np.abs(val_preds - self.val_data[1]) < self.tol)\n        self.val_accuracies.append(val_acc)\n\nbatch_size_update = 16      \nepochs_per_update = 80      \nreplay_sample_size = 16\n\n# We'll record one set of metrics per incremental update.\nmetrics_history = {\n    \"incremental_update\": [],\n    \"avg_loss\": [],\n    \"avg_val_loss\": [],\n    \"avg_accuracy\": [],\n    \"avg_val_accuracy\": [],\n    \"update_time\": []\n}\n\nreplay_buffer = ReplayBuffer(max_size=5000)\n\nnum_training_updates = int(np.ceil(len(training_sequences) / batch_size_update))\nprint(f\"Starting incremental training over {num_training_updates} updates...\\n\")\n\nfor update_idx in range(0, len(training_sequences), batch_size_update):\n    # Get new training samples for the current update.\n    new_sequences = training_sequences[update_idx:update_idx + batch_size_update]\n    new_labels = training_labels[update_idx:update_idx + batch_size_update]\n    update_length = len(new_sequences)\n    \n    # Determine time boundaries for logging (based on training_data timestamps).\n    update_input_start = training_data['Timestamp'].iloc[update_idx]\n    update_input_end = training_data['Timestamp'].iloc[min(update_idx + update_length + sequence_length - 1, len(training_data)-1)]\n    update_label_start = training_data['Timestamp'].iloc[update_idx + sequence_length]\n    update_label_end = training_data['Timestamp'].iloc[min(update_idx + update_length + sequence_length + prediction_length - 1, len(training_data)-1)]\n    \n    current_update = update_idx // batch_size_update + 1\n    print(f\"Processing Incremental update {current_update}/{num_training_updates}:\")\n    print(f\"  - Training Input Hours: from {update_input_start} to {update_input_end}\")\n    print(f\"  - Training Label Hours: from {update_label_start} to {update_label_end}\")\n    \n    # Add new samples to the replay buffer.\n    for seq, lbl in zip(new_sequences, new_labels):\n        replay_buffer.add(seq, lbl)\n    \n    # Sample from the replay buffer to mix with new data.\n    replay_samples = replay_buffer.sample(replay_sample_size)\n    replay_sequences, replay_labels = zip(*replay_samples)\n    replay_sequences = np.array(replay_sequences)\n    replay_labels = np.array(replay_labels)\n    \n    # Combine new and replay data.\n    combined_sequences = np.vstack((new_sequences, replay_sequences))\n    combined_labels = np.vstack((new_labels, replay_labels))\n    \n    # Split combined data into training and validation sets (80/20 split).\n    total_combined = combined_sequences.shape[0]\n    num_val = int(np.ceil(total_combined * 0.2))\n    num_train = total_combined - num_val\n    train_X = combined_sequences[:num_train]\n    train_y = combined_labels[:num_train]\n    val_X = combined_sequences[num_train:]\n    val_y = combined_labels[num_train:]\n    \n    # Create a callback to compute regression accuracy per epoch.\n    acc_callback = RegressionAccuracyCallback(train_data=(train_X, train_y), val_data=(val_X, val_y), tol=0.1)\n    \n    start_time_update = time.time()\n    history = lstm_model.fit(\n        train_X, train_y,\n        batch_size=batch_size_update,\n        epochs=epochs_per_update,\n        verbose=0,\n        validation_data=(val_X, val_y),\n        shuffle=False,\n        callbacks=[acc_callback]\n    )\n    end_time_update = time.time()\n    \n    update_time = end_time_update - start_time_update\n    avg_loss = np.mean(history.history['loss'])\n    avg_val_loss = np.mean(history.history['val_loss'])\n    avg_accuracy = np.mean(acc_callback.train_accuracies)\n    avg_val_accuracy = np.mean(acc_callback.val_accuracies)\n    \n    # Record update-level metrics.\n    metrics_history[\"incremental_update\"].append(current_update)\n    metrics_history[\"avg_loss\"].append(avg_loss)\n    metrics_history[\"avg_val_loss\"].append(avg_val_loss)\n    metrics_history[\"avg_accuracy\"].append(avg_accuracy)\n    metrics_history[\"avg_val_accuracy\"].append(avg_val_accuracy)\n    metrics_history[\"update_time\"].append(update_time)\n    \n    print(f\"Incremental update {current_update}/{num_training_updates}: \"\n          f\"Avg Loss = {avg_loss:.4f}, \"\n          f\"Avg Val Loss = {avg_val_loss:.4f}, \"\n          f\"Update Time = {update_time:.2f}s\\n\")\n\n# Save the trained model.\nmodel_save_path = \"lstm_model.h5\"\nlstm_model.save(model_save_path)\nprint(f\"Model saved to {model_save_path}\\n\")\n\n# ===========================\n# 7. Forecasting and Plotting\n# ===========================\npredicted_values_ratio = lstm_model.predict(np.expand_dims(forecast_sequence, axis=0)).flatten()\nactual_values_ratio = forecast_label.flatten()\n\n# Convert back to actual units.\npredicted_values = predicted_values_ratio * training_mean\nactual_values = actual_values_ratio * training_mean\n\n# Discard fractional parts by converting to integers.\npredicted_values_int = predicted_values.astype(int)\nactual_values_int = actual_values.astype(int)\n\n# Print forecasted values along with the actual ground truth.\nprint(\"Forecasted Values:\")\nprint(predicted_values_int)\nprint(\"Actual Ground Truth:\")\nprint(actual_values_int)\n\nfig, ax = plt.subplots(figsize=(6, 4), dpi=300)\nax.plot(range(prediction_length), actual_values_int, label=\"Actual\", marker=\"*\", linewidth=1, color=\"firebrick\")\nax.plot(range(prediction_length), predicted_values_int, label=\"Forecasted (LSTM-IL)\", linestyle=\"--\", marker=\".\", linewidth=1, color=\"blue\")\nax.set_title(\"Actual vs Predicted Energy Demand (kWh)\", fontsize=12, fontweight=\"bold\")\nax.set_ylabel(\"Energy Consumption (kWh)\", fontsize=10, fontweight=\"bold\")\nax.legend(loc=\"upper right\", fontsize=10)\nax.grid(True, linestyle=\"--\", alpha=0.8)\nax.set_ylim(600, 1400)\nplt.tight_layout()\nplt.savefig(\"Final_Forecasting_LSTM.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 8. Plot Training Loss per Incremental Update\n# ===========================\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_loss\"], label=\"Train Loss\", marker=\"*\", linewidth=1, color=\"crimson\")\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_val_loss\"], label=\"Val Loss\", marker=\".\", linewidth=1, color=\"royalblue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Loss\", fontsize=10, fontweight=\"bold\")\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"LSTM_PM_loss_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 9. Plot Regression Accuracy per Incremental Update\n# ===========================\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_accuracy\"], label=\"Train Accuracy\",  marker=\"*\", linewidth=1, color=\"crimson\")\nplt.plot(metrics_history[\"incremental_update\"], metrics_history[\"avg_val_accuracy\"], label=\"Val Accuracy\", marker=\".\", linewidth=1, color=\"royalblue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Regression Accuracy\", fontsize=10, fontweight=\"bold\")\nplt.legend(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"LSTM_PM_regression_accuracy_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 10. Plot Training Time per Incremental Update (Selected Updates)\n# ===========================\nupdates_arr = np.array(metrics_history[\"incremental_update\"])\nupdate_times = np.array(metrics_history[\"update_time\"])\nselected_indices = np.arange(0, len(updates_arr), 2)  # Adjust the step as needed.\nselected_updates = updates_arr[selected_indices]\nselected_update_times = update_times[selected_indices]\n\nplt.figure(figsize=(6, 4), dpi=300)\nplt.plot(selected_updates, selected_update_times, marker=\"o\", linestyle=\"-\", markersize=5, linewidth=2,\n         color=\"blue\", markerfacecolor=\"white\", markeredgewidth=1.2, markeredgecolor=\"blue\")\nplt.xlabel(\"Incremental update\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Training Time (seconds)\", fontsize=10, fontweight=\"bold\")\nplt.grid(True, linestyle=\"--\", alpha=0.8)\nplt.tight_layout()\nplt.savefig(\"LSTM_PM_Training_Time_per_update.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ===========================\n# 11. Save the Performance Metrics for Later Comparison\n# ===========================\nnp.savez(\"lstm_metrics.npz\",\n         incremental_update=np.array(metrics_history[\"incremental_update\"]),\n         avg_loss=np.array(metrics_history[\"avg_loss\"]),\n         avg_val_loss=np.array(metrics_history[\"avg_val_loss\"]),\n         avg_accuracy=np.array(metrics_history[\"avg_accuracy\"]),\n         avg_val_accuracy=np.array(metrics_history[\"avg_val_accuracy\"]),\n         update_time=np.array(metrics_history[\"update_time\"]),\n         forecast_pred=np.array(predicted_values),\n         actual=np.array(actual_values))\nprint(\"LSTM model metrics saved to lstm_metrics.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:32:30.652120Z","iopub.execute_input":"2025-04-13T13:32:30.652347Z","iopub.status.idle":"2025-04-13T13:37:59.707274Z","shell.execute_reply.started":"2025-04-13T13:32:30.652328Z","shell.execute_reply":"2025-04-13T13:37:59.706582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**xLSTM Modeling Importing**","metadata":{}},{"cell_type":"code","source":"# Clone the XLSTM repository\n!git clone https://github.com/NX-AI/xlstm.git\n\n# Navigate to the xlstm directory\n%cd xlstm\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:37:59.709033Z","iopub.execute_input":"2025-04-13T13:37:59.709258Z","iopub.status.idle":"2025-04-13T13:38:00.798727Z","shell.execute_reply.started":"2025-04-13T13:37:59.709240Z","shell.execute_reply":"2025-04-13T13:38:00.797663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install PyTorch\n!pip install torch torchvision torchaudio\n\n# Install additional dependencies\n!pip install numpy pandas scikit-learn matplotlib\n\n# Install the xLSTM package\n!pip install xlstm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:00.799897Z","iopub.execute_input":"2025-04-13T13:38:00.800226Z","iopub.status.idle":"2025-04-13T13:38:13.645984Z","shell.execute_reply.started":"2025-04-13T13:38:00.800200Z","shell.execute_reply":"2025-04-13T13:38:13.644965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install seaborn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:13.647024Z","iopub.execute_input":"2025-04-13T13:38:13.647259Z","iopub.status.idle":"2025-04-13T13:38:13.651224Z","shell.execute_reply.started":"2025-04-13T13:38:13.647239Z","shell.execute_reply":"2025-04-13T13:38:13.650503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xlstm.xlstm.blocks.mlstm.layer import mLSTMLayerConfig\nfrom xlstm.xlstm.blocks.slstm.layer import sLSTMLayerConfig\nfrom xlstm.xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\nfrom xlstm.xlstm.blocks.mlstm.block import mLSTMBlockConfig\nfrom xlstm.xlstm.blocks.slstm.block import sLSTMBlockConfig\nfrom xlstm.xlstm.components.feedforward import FeedForwardConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:13.652135Z","iopub.execute_input":"2025-04-13T13:38:13.652361Z","iopub.status.idle":"2025-04-13T13:38:18.315032Z","shell.execute_reply.started":"2025-04-13T13:38:13.652342Z","shell.execute_reply":"2025-04-13T13:38:18.314064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install ninja-build -y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:18.315940Z","iopub.execute_input":"2025-04-13T13:38:18.316166Z","iopub.status.idle":"2025-04-13T13:38:25.007350Z","shell.execute_reply.started":"2025-04-13T13:38:18.316148Z","shell.execute_reply":"2025-04-13T13:38:25.006510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ninja --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:25.008504Z","iopub.execute_input":"2025-04-13T13:38:25.008766Z","iopub.status.idle":"2025-04-13T13:38:25.216026Z","shell.execute_reply.started":"2025-04-13T13:38:25.008744Z","shell.execute_reply":"2025-04-13T13:38:25.214886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:25.217529Z","iopub.execute_input":"2025-04-13T13:38:25.217947Z","iopub.status.idle":"2025-04-13T13:38:25.432232Z","shell.execute_reply.started":"2025-04-13T13:38:25.217914Z","shell.execute_reply":"2025-04-13T13:38:25.431361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!rm -rf /root/.cache/torch_extensions/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:25.433325Z","iopub.execute_input":"2025-04-13T13:38:25.433591Z","iopub.status.idle":"2025-04-13T13:38:25.437438Z","shell.execute_reply.started":"2025-04-13T13:38:25.433568Z","shell.execute_reply":"2025-04-13T13:38:25.436437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:25.438335Z","iopub.execute_input":"2025-04-13T13:38:25.438640Z","iopub.status.idle":"2025-04-13T13:38:25.453179Z","shell.execute_reply.started":"2025-04-13T13:38:25.438615Z","shell.execute_reply":"2025-04-13T13:38:25.452488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:25.454016Z","iopub.execute_input":"2025-04-13T13:38:25.454283Z","iopub.status.idle":"2025-04-13T13:38:28.789629Z","shell.execute_reply.started":"2025-04-13T13:38:25.454263Z","shell.execute_reply":"2025-04-13T13:38:28.788567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchinfo\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:28.790716Z","iopub.execute_input":"2025-04-13T13:38:28.791018Z","iopub.status.idle":"2025-04-13T13:38:32.141188Z","shell.execute_reply.started":"2025-04-13T13:38:28.790987Z","shell.execute_reply":"2025-04-13T13:38:32.139863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    from torchinfo import summary  # For model summary\nexcept ImportError:\n    !pip install torchinfo\n    from torchinfo import summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:32.142428Z","iopub.execute_input":"2025-04-13T13:38:32.142797Z","iopub.status.idle":"2025-04-13T13:38:32.169121Z","shell.execute_reply.started":"2025-04-13T13:38:32.142760Z","shell.execute_reply":"2025-04-13T13:38:32.168445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. xLSTM Incremental Learning Model","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, Subset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom torchinfo import summary\n\n# Import xLSTM components\nfrom xlstm.xlstm.blocks.slstm.layer import sLSTMLayerConfig\nfrom xlstm.xlstm.blocks.slstm.block import sLSTMBlockConfig\nfrom xlstm.xlstm.blocks.mlstm.layer import mLSTMLayerConfig\nfrom xlstm.xlstm.blocks.mlstm.block import mLSTMBlockConfig\nfrom xlstm.xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\n\n# =============================================================================\n# 1. Data Loading, Early Splitting, and Preprocessing (Mean-Based Scaling)\n# =============================================================================\ndef load_energy_dataset(file_path, sequence_length=48, prediction_length=48):\n    # Read CSV and preprocess timestamps.\n    data = pd.read_csv(file_path)\n    data['Datetime'] = pd.to_datetime(data['Timestamp'])\n    # Drop unused columns and sort by time.\n    data = data.drop(columns=['Timestamp', 'Weather_Condition_x', 'Weather_Condition_y'])\n    data = data.sort_values(by='Datetime').reset_index(drop=True)\n    numeric_columns = ['Energy_Demand', 'Energy_Supply', 'Temperature', 'Grid_Load',\n                       'Renewable_Source_Output', 'NonRenewable_Source_Output', 'Energy_Price']\n    data = data[['Datetime'] + numeric_columns]\n    # Resample hourly and forward fill missing values.\n    data_hourly = data.set_index('Datetime').resample('h').mean().ffill()\n    \n    # Early Split: Reserve the last prediction_length hours for forecasting.\n    total_rows = len(data_hourly)\n    n_forecast = prediction_length  # e.g., last 48 rows reserved for forecasting\n    n_train = total_rows - n_forecast\n    train_data = data_hourly.iloc[:n_train]\n    forecast_data = data_hourly.iloc[n_train:]\n    \n    print(f\"Total records in resampled dataset: {total_rows}\")\n    print(f\"Training records: {n_train}\")\n    print(f\"Forecast records: {n_forecast}\")\n    \n    # Scale 'Energy_Demand' using mean-based scaling (divide by training mean).\n    energy_demand_train = train_data['Energy_Demand'].values.reshape(-1, 1)\n    training_mean = energy_demand_train.mean()  # Compute training mean\n    train_scaled = energy_demand_train / training_mean\n\n    # Scale forecast data using the same training mean.\n    energy_demand_forecast = forecast_data['Energy_Demand'].values.reshape(-1, 1)\n    forecast_scaled = energy_demand_forecast / training_mean\n    \n    # Generate sliding-window sequences from training data.\n    X, y = [], []\n    num_sequences = len(train_scaled) - sequence_length - prediction_length + 1\n    for i in range(num_sequences):\n        X.append(train_scaled[i : i + sequence_length])\n        y.append(train_scaled[i + sequence_length : i + sequence_length + prediction_length])\n    X = torch.tensor(X, dtype=torch.float32)  # shape: (num_sequences, sequence_length, 1)\n    y = torch.tensor(y, dtype=torch.float32).squeeze(-1)  # shape: (num_sequences, prediction_length)\n    \n    print(f\"Total training sequences generated: {len(X)}\")\n    \n    # Create forecast sequence from the last sequence_length hours of training data.\n    forecast_sequence = train_scaled[-sequence_length:]\n    forecast_sequence = torch.tensor(forecast_sequence, dtype=torch.float32)\n    \n    # Forecast labels from reserved forecast data.\n    forecast_labels = torch.tensor(forecast_scaled, dtype=torch.float32).squeeze(-1)\n    \n    train_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n    return train_loader, training_mean, train_data, forecast_sequence, forecast_labels\n\n# =============================================================================\n# 2. Split Data into Chunks for Incremental Training\n# =============================================================================\ndef split_data_into_chunks(data_loader, chunk_size):\n    data_chunks = []\n    X, y = data_loader.dataset.tensors\n    for i in range(0, len(X), chunk_size):\n        chunk_X = X[i : i + chunk_size]\n        chunk_y = y[i : i + chunk_size]\n        if len(chunk_X) > 0:\n            data_chunks.append((i, DataLoader(TensorDataset(chunk_X, chunk_y), batch_size=32, shuffle=True)))\n    return data_chunks\n\n# =============================================================================\n# 3. Define the XLSTM Model (Reduced Capacity)\n# =============================================================================\nclass XLSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, slstm_blocks=1, mlstm_blocks=1):\n        super(XLSTMModel, self).__init__()\n        self.input_embedding = nn.Linear(input_size, hidden_size)\n        self.blocks = nn.ModuleList()\n        block_idx = 0\n        # Add one sLSTM block.\n        while slstm_blocks > 0:\n            slstm_layer_config = sLSTMLayerConfig(\n                hidden_size=hidden_size,\n                num_heads=4,\n                num_states=3,\n                backend='cuda',\n                function='slstm',\n                dropout=0.4\n            )\n            slstm_block_config = sLSTMBlockConfig(\n                slstm=slstm_layer_config,\n                _num_blocks=1,\n                _block_idx=block_idx\n            )\n            self.blocks.append(\n                xLSTMBlockStack(config=xLSTMBlockStackConfig(\n                    slstm_block=slstm_block_config,\n                    num_blocks=1,\n                    context_length=48,\n                    embedding_dim=hidden_size,\n                    dropout=0.4,\n                    add_post_blocks_norm=False\n                ))\n            )\n            slstm_blocks -= 1\n            block_idx += 1\n        # Add one mLSTM block.\n        while mlstm_blocks > 0:\n            mlstm_layer_config = mLSTMLayerConfig(\n                num_heads=1,\n                embedding_dim=hidden_size,\n                dropout=0.4\n            )\n            mlstm_block_config = mLSTMBlockConfig(\n                mlstm=mlstm_layer_config,\n                _num_blocks=1,\n                _block_idx=block_idx\n            )\n            self.blocks.append(\n                xLSTMBlockStack(config=xLSTMBlockStackConfig(\n                    mlstm_block=mlstm_block_config,\n                    num_blocks=1,\n                    context_length=48,\n                    embedding_dim=hidden_size,\n                    dropout=0.4,\n                    add_post_blocks_norm=False\n                ))\n            )\n            mlstm_blocks -= 1\n            block_idx += 1\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x: (batch_size, context_length, input_size)\n        x = self.input_embedding(x)  # (batch_size, 48, hidden_size)\n        for block in self.blocks:\n            x = block(x)\n        # Use the final time step for prediction.\n        x = self.fc(x[:, -1, :])\n        return x\n\n# =============================================================================\n# 4. Regression Accuracy Computation (Tolerance-Based)\n# =============================================================================\ndef compute_regression_accuracy(model, loader, tol, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    with torch.no_grad():\n        for batch_X, batch_y in loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            preds = model(batch_X)\n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(batch_y.cpu().numpy())\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n    return np.mean(np.abs(all_preds - all_targets) < tol)\n\n# =============================================================================\n# 5. Incremental Training Functions\n# =============================================================================\ndef train_one_epoch(model, train_loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch_X, batch_y in loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            total_loss += loss.item()\n    return total_loss / len(loader) if len(loader) > 0 else 0\n\ndef split_train_val(chunk_loader, val_ratio=0.1):\n    dataset = chunk_loader.dataset\n    total_samples = len(dataset)\n    indices = list(range(total_samples))\n    split = int(val_ratio * total_samples)\n    if total_samples > 1 and split < 1:\n        split = 1\n    random.shuffle(indices)\n    val_indices = indices[:split]\n    train_indices = indices[split:]\n    if len(train_indices) == 0:\n        train_indices = indices\n        val_indices = indices\n    train_subset = Subset(dataset, train_indices)\n    val_subset = Subset(dataset, val_indices)\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n    return train_loader, val_loader\n\n# =============================================================================\n# 6. Main Incremental Training Script for xLSTM Model (Tracking Performance per Update)\n# =============================================================================\nif __name__ == '__main__':\n    file_path = '/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv'\n    # load_energy_dataset returns: train_loader, training_mean, train_data, forecast_sequence, forecast_labels.\n    train_loader, training_mean, train_data, forecast_sequence, forecast_labels = load_energy_dataset(\n        file_path, sequence_length=48, prediction_length=48\n    )\n    chunk_size = 16  # Number of new samples per incremental update.\n    data_chunks = split_data_into_chunks(train_loader, chunk_size=chunk_size)\n    total_chunks = len(data_chunks)\n    print(f\"Total number of incremental updates (chunks): {total_chunks}\")\n\n    # Model configuration (reduced capacity to match previous models)\n    input_size = 1\n    hidden_size = 128       # To match comparable capacity.\n    output_size = 48        # Prediction horizon.\n    slstm_blocks = 1        # 1 sLSTM block.\n    mlstm_blocks = 1        # 1 mLSTM block.\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    modelxlstm = XLSTMModel(input_size, hidden_size, output_size,\n                            slstm_blocks=slstm_blocks, mlstm_blocks=mlstm_blocks).to(device)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(modelxlstm.parameters(), lr=0.001)\n\n    # Print model summary using dummy input.\n    print(summary(modelxlstm, input_size=(32, 48, 1)))\n\n    # Training configuration: local epochs per update and tolerance.\n    chunk_epochs = 80\n    tol = 0.1\n\n    # Metrics history dictionary for incremental updates.\n    metrics_history = {\n        \"incremental_update\": [],\n        \"avg_loss\": [],\n        \"avg_val_loss\": [],\n        \"avg_accuracy\": [],\n        \"avg_val_accuracy\": [],\n        \"update_time\": []\n    }\n\n    print(\"Starting incremental training on xLSTM model (tracking performance per update)...\")\n    update_count = 0\n\n    for chunk_index, (chunk_start_idx, chunk_loader) in enumerate(data_chunks):\n        chunk_len = len(chunk_loader.dataset)\n        # Determine time window from train_data index.\n        start_time = train_data.index[chunk_start_idx]\n        end_idx = chunk_start_idx + chunk_len + 48 - 1\n        end_time = train_data.index[end_idx] if end_idx < len(train_data.index) else train_data.index[-1]\n        print(f\"\\nUpdate {chunk_index+1}/{total_chunks} - New Data: {chunk_len} samples covering hours from {start_time} to {end_time}\")\n        \n        train_chunk_loader, val_loader = split_train_val(chunk_loader, val_ratio=0.1)\n        epoch_train_losses = []\n        epoch_val_losses = []\n        epoch_train_accuracies = []\n        epoch_val_accuracies = []\n        update_start_time = time.time()\n        \n        for epoch in range(chunk_epochs):\n            start_epoch = time.time()\n            train_loss = train_one_epoch(modelxlstm, train_chunk_loader, optimizer, criterion, device)\n            val_loss = evaluate(modelxlstm, val_loader, criterion, device)\n            train_acc = compute_regression_accuracy(modelxlstm, train_chunk_loader, tol, device)\n            val_acc = compute_regression_accuracy(modelxlstm, val_loader, tol, device)\n            epoch_train_losses.append(train_loss)\n            epoch_val_losses.append(val_loss)\n            epoch_train_accuracies.append(train_acc)\n            epoch_val_accuracies.append(val_acc)\n            end_epoch = time.time()\n            # Uncomment the following line if you need per-epoch reporting.\n            # print(f\"  [Update {chunk_index+1}] Epoch {epoch+1}/{chunk_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}, Epoch Time={(end_epoch-start_epoch):.2f}s\")\n        \n        update_end_time = time.time()\n        update_time = update_end_time - update_start_time\n        update_count += 1\n        avg_train_loss = np.mean(epoch_train_losses)\n        avg_val_loss = np.mean(epoch_val_losses)\n        avg_train_acc = np.mean(epoch_train_accuracies)\n        avg_val_acc = np.mean(epoch_val_accuracies)\n        \n        metrics_history[\"incremental_update\"].append(update_count)\n        metrics_history[\"avg_loss\"].append(avg_train_loss)\n        metrics_history[\"avg_val_loss\"].append(avg_val_loss)\n        metrics_history[\"avg_accuracy\"].append(avg_train_acc)\n        metrics_history[\"avg_val_accuracy\"].append(avg_val_acc)\n        metrics_history[\"update_time\"].append(update_time)\n        \n        print(f\"Update {update_count} complete: Avg Train Loss={avg_train_loss:.4f}, Avg Val Loss={avg_val_loss:.4f}, \"\n              f\"Avg Train Acc={avg_train_acc:.4f}, Avg Val Acc={avg_val_acc:.4f}, Update Time={update_time:.2f}s\")\n        \n        modelxlstm.eval()\n        all_preds, all_targets = [], []\n        with torch.no_grad():\n            for batch_X, batch_y in chunk_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                preds = modelxlstm(batch_X).cpu().numpy()\n                all_preds.extend(preds)\n                all_targets.extend(batch_y.cpu().numpy())\n        mae = mean_absolute_error(all_targets, all_preds)\n        mse = mean_squared_error(all_targets, all_preds)\n        r2 = r2_score(all_targets, all_preds)\n        print(f\"Update {update_count} Evaluation - MAE: {mae:.4f}, MSE: {mse:.4f}, R: {r2:.4f}\")\n\n    print(\"Incremental training complete.\\n\")\n\n    # =============================================================================\n    # 7. Forecasting Visualization: Compare Actual vs. Predicted Sequences (Original Units)\n    # =============================================================================\n    modelxlstm.eval()\n    with torch.no_grad():\n        predicted_values = modelxlstm(forecast_sequence.unsqueeze(0).to(device)).cpu().numpy().flatten()\n    actual_values = forecast_labels.flatten().numpy()\n    # Convert normalized forecasts back to original energy consumption (kWh).\n    predicted_values = predicted_values * training_mean\n    actual_values = actual_values * training_mean\n\n    fig, ax = plt.subplots(figsize=(8, 5), dpi=300)\n    ax.plot(range(48), actual_values, label=\"Actual\", color=\"firebrick\", marker=\"*\", linewidth=1)\n    ax.plot(range(48), predicted_values, label=\"Forecast (XLSTM)\", color=\"blue\", linestyle=\"--\", marker=\".\", linewidth=1)\n    ax.set_title(\"Actual vs. Predicted Energy Demand (kWh)\", fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Time Steps (Hours)\", fontsize=10, fontweight=\"bold\")\n    ax.set_ylabel(\"Energy Consumption (kWh)\", fontsize=10, fontweight=\"bold\")\n    ax.legend(loc=\"upper right\", fontsize=10)\n    ax.grid(True, linestyle=\"--\", alpha=0.8)\n    plt.tight_layout()\n    plt.savefig(\"Final_Forecasting_XLSTM.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n    # =============================================================================\n    # 8. Plot Training and Validation Loss per Update\n    # =============================================================================\n    updates_arr = np.array(metrics_history[\"incremental_update\"])\n    plt.figure(figsize=(6, 4), dpi=300)\n    plt.plot(updates_arr, metrics_history[\"avg_loss\"], label=\"Train Loss\", color=\"crimson\", linewidth=1)\n    plt.plot(updates_arr, metrics_history[\"avg_val_loss\"], label=\"Validation Loss\", color=\"royalblue\", linewidth=1)\n    plt.title(\"XLSTM Model: Training and Validation Loss per Update\", fontsize=10, fontweight=\"bold\")\n    plt.xlabel(\"Incremental Update\", fontsize=10, fontweight=\"bold\")\n    plt.ylabel(\"Loss\", fontsize=10, fontweight=\"bold\")\n    plt.legend(fontsize=10)\n    plt.grid(True, linestyle=\"--\", alpha=0.8)\n    plt.tight_layout()\n    plt.savefig(\"XLSTM_loss_per_update.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n    # =============================================================================\n    # 9. Plot Regression Accuracy per Update (Tolerance-Based)\n    # =============================================================================\n    plt.figure(figsize=(6, 4), dpi=300)\n    plt.plot(updates_arr, metrics_history[\"avg_accuracy\"], label=\"Train Accuracy\", color=\"crimson\", linewidth=1)\n    plt.plot(updates_arr, metrics_history[\"avg_val_accuracy\"], label=\"Validation Accuracy\", color=\"royalblue\", linewidth=1)\n    plt.title(\"XLSTM Model: Training and Validation Regression Accuracy per Update\", fontsize=10, fontweight=\"bold\")\n    plt.xlabel(\"Incremental Update\", fontsize=10, fontweight=\"bold\")\n    plt.ylabel(\"Regression Accuracy\", fontsize=10, fontweight=\"bold\")\n    plt.legend(fontsize=10)\n    plt.grid(True, linestyle=\"--\", alpha=0.8)\n    plt.tight_layout()\n    plt.savefig(\"XLSTM_regression_accuracy_per_update.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n    # =============================================================================\n    # 10. Plot Training Time per Incremental Update (Selected Updates)\n    # =============================================================================\n    update_times_arr = np.array(metrics_history[\"update_time\"])\n    selected_indices = np.arange(0, len(updates_arr), 2)  # Adjust step size as needed.\n    selected_updates = updates_arr[selected_indices]\n    selected_update_times = update_times_arr[selected_indices]\n\n    plt.figure(figsize=(6, 4), dpi=300)\n    plt.plot(selected_updates, selected_update_times, marker=\"o\", linestyle=\"-\", markersize=5, linewidth=2,\n             color=\"blue\", markerfacecolor=\"white\", markeredgewidth=1.2, markeredgecolor=\"blue\")\n    plt.xlabel(\"Incremental Update\", fontsize=10, fontweight=\"bold\")\n    plt.ylabel(\"Training Time (seconds)\", fontsize=10, fontweight=\"bold\")\n    plt.title(\"XLSTM Model: Training Time per Incremental Update\", fontsize=10, fontweight=\"bold\", pad=5)\n    plt.grid(True, linestyle=\"--\", alpha=0.8)\n    plt.tight_layout()\n    plt.savefig(\"XLSTM_Training_Time_per_update.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n    # =============================================================================\n    # 11. Save the Trained XLSTM Model and Performance Metrics\n    # =============================================================================\n\n\n    os.chdir('/kaggle/working/')\n    model_save_path = \"xlstm_model_incremental.pt\"\n    torch.save(modelxlstm.state_dict(), model_save_path)\n    print(f\"XLSTM model saved to {model_save_path}\")\n    os.chdir('/kaggle/working/')\n    np.savez(\"xlstm_metrics.npz\",\n             incremental_update=np.array(metrics_history[\"incremental_update\"]),\n             avg_loss=np.array(metrics_history[\"avg_loss\"]),\n             avg_val_loss=np.array(metrics_history[\"avg_val_loss\"]),\n             avg_accuracy=np.array(metrics_history[\"avg_accuracy\"]),\n             avg_val_accuracy=np.array(metrics_history[\"avg_val_accuracy\"]),\n             update_time=np.array(metrics_history[\"update_time\"]),\n             forecast_pred=np.array(predicted_values),\n             actual=np.array(actual_values))\n    print(\"XLSTM model metrics saved to xlstm_metrics.npz\")\n\n\n\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:38:32.169960Z","iopub.execute_input":"2025-04-13T13:38:32.170209Z","iopub.status.idle":"2025-04-13T13:40:02.928538Z","shell.execute_reply.started":"2025-04-13T13:38:32.170188Z","shell.execute_reply":"2025-04-13T13:40:02.927637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparison of all models","metadata":{}},{"cell_type":"code","source":"#os.chdir('/kaggle/working/')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:02.929565Z","iopub.execute_input":"2025-04-13T13:40:02.930329Z","iopub.status.idle":"2025-04-13T13:40:02.933769Z","shell.execute_reply.started":"2025-04-13T13:40:02.930290Z","shell.execute_reply":"2025-04-13T13:40:02.932859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#os.chdir('/kaggle/working/')\n#import os\nprint(os.getcwd())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:02.937371Z","iopub.execute_input":"2025-04-13T13:40:02.937693Z","iopub.status.idle":"2025-04-13T13:40:02.946962Z","shell.execute_reply.started":"2025-04-13T13:40:02.937661Z","shell.execute_reply":"2025-04-13T13:40:02.946073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import numpy as np\nimport matplotlib.pyplot as plt\n\n# ---------------------------\n# Load Metrics for Each Model\n# ---------------------------\nhybrid_data = np.load(\"hybrid_metrics.npz\")\ncnn_data    = np.load(\"cnn_metrics.npz\")\nlstm_data   = np.load(\"lstm_metrics.npz\")\nxlstm_data  = np.load(\"xlstm_metrics.npz\")\n\n# For Hybrid, CNN, and incremental LSTM, use:\n#  - incremental_update: global update number\n#  - avg_loss and avg_val_loss: training/validation loss\n#  - avg_accuracy and avg_val_accuracy: training/validation accuracy\n#  - update_time: training time per update\n#  - forecast_pred and actual: forecast predictions and ground truth\n\n# Extract variables for the Hybrid model\nhybrid_updates       = hybrid_data[\"incremental_update\"]\nhybrid_loss          = hybrid_data[\"avg_loss\"]\nhybrid_val_loss      = hybrid_data[\"avg_val_loss\"]\nhybrid_accuracy      = hybrid_data[\"avg_accuracy\"]\nhybrid_val_accuracy  = hybrid_data[\"avg_val_accuracy\"]\nhybrid_update_times  = hybrid_data[\"update_time\"]\nhybrid_forecast      = hybrid_data[\"forecast_pred\"]\nhybrid_actual        = hybrid_data[\"actual\"]\n\n# Extract variables for the CNN model\ncnn_updates       = cnn_data[\"incremental_update\"]\ncnn_loss          = cnn_data[\"avg_loss\"]\ncnn_val_loss      = cnn_data[\"avg_val_loss\"]\ncnn_accuracy      = cnn_data[\"avg_accuracy\"]\ncnn_val_accuracy  = cnn_data[\"avg_val_accuracy\"]\ncnn_update_times  = cnn_data[\"update_time\"]\ncnn_forecast      = cnn_data[\"forecast_pred\"]\ncnn_actual        = cnn_data[\"actual\"]\n\n# Extract variables for the Incremental LSTM model\nlstm_updates       = lstm_data[\"incremental_update\"]\nlstm_loss          = lstm_data[\"avg_loss\"]\nlstm_val_loss      = lstm_data[\"avg_val_loss\"]\nlstm_accuracy      = lstm_data[\"avg_accuracy\"]\nlstm_val_accuracy  = lstm_data[\"avg_val_accuracy\"]\nlstm_update_times  = lstm_data[\"update_time\"]\nlstm_forecast      = lstm_data[\"forecast_pred\"]\nlstm_actual        = lstm_data[\"actual\"]\n\n# Extract variables for the XLSTM model (using the same keys)\nxlstm_updates       = xlstm_data[\"incremental_update\"]\nxlstm_loss          = xlstm_data[\"avg_loss\"]\nxlstm_val_loss      = xlstm_data[\"avg_val_loss\"]\nxlstm_accuracy      = xlstm_data[\"avg_accuracy\"]\nxlstm_val_accuracy  = xlstm_data[\"avg_val_accuracy\"]\nxlstm_update_times  = xlstm_data[\"update_time\"]\nxlstm_forecast      = xlstm_data[\"forecast_pred\"]\nxlstm_actual        = xlstm_data[\"actual\"]\n\n# For forecasting, we assume that the actual forecast target is identical across models.\nactual_forecast = hybrid_actual  # shape: (48,)\n\n# ---------------------------\n# Create Subplots for Comparison\n# ---------------------------\nfig, axs = plt.subplots(2, 2, figsize=(16, 14))\n\n# --- Subplot 1: Training and Validation Loss vs. Incremental Update ---\naxs[0, 0].plot(hybrid_updates, hybrid_loss, label='iTBG-Net Train Loss', color='blue')\naxs[0, 0].plot(hybrid_updates, hybrid_val_loss, label='iTBG-Net Val Loss', color='blue', linestyle='--')\naxs[0, 0].plot(cnn_updates, cnn_loss, label='CNN Train Loss', color='green')\naxs[0, 0].plot(cnn_updates, cnn_val_loss, label='CNN Val Loss', color='green', linestyle='--')\naxs[0, 0].plot(lstm_updates, lstm_loss, label='LSTM Train Loss', color='red')\naxs[0, 0].plot(lstm_updates, lstm_val_loss, label='LSTM Val Loss', color='red', linestyle='--')\naxs[0, 0].plot(xlstm_updates, xlstm_loss, label='XLSTM Train Loss', color='purple')\naxs[0, 0].plot(xlstm_updates, xlstm_val_loss, label='XLSTM Val Loss', color='purple', linestyle='--')\n\naxs[0, 0].set_title('Training & Validation Loss')\naxs[0, 0].set_xlabel('Incremental Update')\naxs[0, 0].set_ylabel('Loss')\naxs[0, 0].legend()\naxs[0, 0].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 2: Training and Validation Accuracy vs. Incremental Update ---\naxs[0, 1].plot(hybrid_updates, hybrid_accuracy, label='iTBG-Net Train Accuracy', color='blue')\naxs[0, 1].plot(hybrid_updates, hybrid_val_accuracy, label='iTBG-Net Val Accuracy', color='blue', linestyle='--')\naxs[0, 1].plot(cnn_updates, cnn_accuracy, label='CNN Train Accuracy', color='green')\naxs[0, 1].plot(cnn_updates, cnn_val_accuracy, label='CNN Val Accuracy', color='green', linestyle='--')\naxs[0, 1].plot(lstm_updates, lstm_accuracy, label='LSTM Train Accuracy', color='red')\naxs[0, 1].plot(lstm_updates, lstm_val_accuracy, label='LSTM Val Accuracy', color='red', linestyle='--')\naxs[0, 1].plot(xlstm_updates, xlstm_accuracy, label='XLSTM Train Accuracy', color='purple')\naxs[0, 1].plot(xlstm_updates, xlstm_val_accuracy, label='XLSTM Val Accuracy', color='purple', linestyle='--')\n\naxs[0, 1].set_title('Training & Validation Accuracy')\naxs[0, 1].set_xlabel('Incremental Update')\naxs[0, 1].set_ylabel('Regression Accuracy')\naxs[0, 1].legend()\naxs[0, 1].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 3: Training Time per Incremental Update ---\naxs[1, 0].plot(hybrid_updates, hybrid_update_times, label='iTBG-Net', color='blue')\naxs[1, 0].plot(cnn_updates, cnn_update_times, label='CNN', color='green')\naxs[1, 0].plot(lstm_updates, lstm_update_times, label='LSTM', color='red')\naxs[1, 0].plot(xlstm_updates, xlstm_update_times, label='XLSTM', color='purple')\n\naxs[1, 0].set_title('Training Time per Incremental Update')\naxs[1, 0].set_xlabel('Incremental Update')\naxs[1, 0].set_ylabel('Time (seconds)')\naxs[1, 0].legend()\naxs[1, 0].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 4: Forecasting Comparison (48-step) ---\ntime_steps = np.arange(48)\naxs[1, 1].plot(time_steps, actual_forecast, label='Actual', color='black', marker='o')\naxs[1, 1].plot(time_steps, hybrid_forecast, label='iTBG-Net Forecast', color='blue', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, cnn_forecast, label='CNN Forecast', color='green', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, lstm_forecast, label='LSTM Forecast', color='red', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, xlstm_forecast, label='XLSTM Forecast', color='purple', linestyle='--', marker='s')\n\naxs[1, 1].set_title('48-step Forecasting Comparison')\naxs[1, 1].set_xlabel('Time Step')\naxs[1, 1].set_ylabel('Energy Demand')\naxs[1, 1].legend()\naxs[1, 1].grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.savefig(\"model_comparison.png\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:02.948395Z","iopub.execute_input":"2025-04-13T13:40:02.948692Z","iopub.status.idle":"2025-04-13T13:40:02.960804Z","shell.execute_reply.started":"2025-04-13T13:40:02.948671Z","shell.execute_reply":"2025-04-13T13:40:02.960043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above results show that after some time, incremental learning keeps learning and batch-based learning stops learning, and after a while it keeps the lines straight, which means the model doesn't have more capacity to learn new data points, but the streaming model also keeps the old training and learning new data points, which is a good gesture to the natural learning process.","metadata":{}},{"cell_type":"markdown","source":"# Comparison of Models","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# Load Metrics for Each Model\n# -----------------------------\nhybrid_data = np.load(\"hybrid_metrics.npz\")\ncnn_data    = np.load(\"cnn_metrics.npz\")\nlstm_data   = np.load(\"lstm_metrics.npz\")\nxlstm_data  = np.load(\"xlstm_metrics.npz\")\n\n# For Hybrid, CNN, and incremental LSTM, the keys are:\n#   incremental_update, avg_loss, avg_val_loss, avg_accuracy, avg_val_accuracy, update_time, forecast_pred, actual\n\n# Extract variables for the Hybrid model.\nhybrid_updates       = hybrid_data[\"incremental_update\"]\nhybrid_loss          = hybrid_data[\"avg_loss\"]\nhybrid_val_loss      = hybrid_data[\"avg_val_loss\"]\nhybrid_accuracy      = hybrid_data[\"avg_accuracy\"]\nhybrid_val_accuracy  = hybrid_data[\"avg_val_accuracy\"]\nhybrid_update_times  = hybrid_data[\"update_time\"]\nhybrid_forecast      = hybrid_data[\"forecast_pred\"]\nhybrid_actual        = hybrid_data[\"actual\"]\n\n# Extract variables for the CNN model.\ncnn_updates       = cnn_data[\"incremental_update\"]\ncnn_loss          = cnn_data[\"avg_loss\"]\ncnn_val_loss      = cnn_data[\"avg_val_loss\"]\ncnn_accuracy      = cnn_data[\"avg_accuracy\"]\ncnn_val_accuracy  = cnn_data[\"avg_val_accuracy\"]\ncnn_update_times  = cnn_data[\"update_time\"]\ncnn_forecast      = cnn_data[\"forecast_pred\"]\ncnn_actual        = cnn_data[\"actual\"]\n\n# Extract variables for the incremental LSTM model.\nlstm_updates       = lstm_data[\"incremental_update\"]\nlstm_loss          = lstm_data[\"avg_loss\"]\nlstm_val_loss      = lstm_data[\"avg_val_loss\"]\nlstm_accuracy      = lstm_data[\"avg_accuracy\"]\nlstm_val_accuracy  = lstm_data[\"avg_val_accuracy\"]\nlstm_update_times  = lstm_data[\"update_time\"]\nlstm_forecast      = lstm_data[\"forecast_pred\"]\nlstm_actual        = lstm_data[\"actual\"]\n\n# For the xLSTM model, we now also use the same keys.\nxlstm_updates       = xlstm_data[\"incremental_update\"]\nxlstm_loss          = xlstm_data[\"avg_loss\"]\nxlstm_val_loss      = xlstm_data[\"avg_val_loss\"]\nxlstm_accuracy      = xlstm_data[\"avg_accuracy\"]\nxlstm_val_accuracy  = xlstm_data[\"avg_val_accuracy\"]\nxlstm_update_times  = xlstm_data[\"update_time\"]\nxlstm_forecast      = xlstm_data[\"forecast_pred\"]\nxlstm_actual        = xlstm_data[\"actual\"]\n\n# For forecasting, assume that the actual target is identical across models.\nactual_forecast = hybrid_actual  # shape: (48,)\n\n# -----------------------------\n# Create Subplots for Comparison\n# -----------------------------\nfig, axs = plt.subplots(2, 2, figsize=(16, 14))\n\n# --- Subplot 1: Training and Validation Loss vs. Incremental Update ---\naxs[0, 0].plot(hybrid_updates, hybrid_loss, label='iTBG-Net Train Loss', color='blue')\naxs[0, 0].plot(hybrid_updates, hybrid_val_loss, label='iTBG-Net Val Loss', color='blue', linestyle='--')\naxs[0, 0].plot(cnn_updates, cnn_loss, label='CNN Train Loss', color='green')\naxs[0, 0].plot(cnn_updates, cnn_val_loss, label='CNN Val Loss', color='green', linestyle='--')\naxs[0, 0].plot(lstm_updates, lstm_loss, label='LSTM Train Loss', color='red')\naxs[0, 0].plot(lstm_updates, lstm_val_loss, label='LSTM Val Loss', color='red', linestyle='--')\naxs[0, 0].plot(xlstm_updates, xlstm_loss, label='XLSTM Train Loss', color='purple')\naxs[0, 0].plot(xlstm_updates, xlstm_val_loss, label='XLSTM Val Loss', color='purple', linestyle='--')\naxs[0, 0].set_title('Training & Validation Loss')\naxs[0, 0].set_xlabel('Incremental Update')\naxs[0, 0].set_ylabel('Loss')\naxs[0, 0].legend(fontsize=9)\naxs[0, 0].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 2: Training and Validation Accuracy vs. Incremental Update ---\naxs[0, 1].plot(hybrid_updates, hybrid_accuracy, label='iTBG-Net Train Accuracy', color='blue')\naxs[0, 1].plot(hybrid_updates, hybrid_val_accuracy, label='iTBG-Net Val Accuracy', color='blue', linestyle='--')\naxs[0, 1].plot(cnn_updates, cnn_accuracy, label='CNN Train Accuracy', color='green')\naxs[0, 1].plot(cnn_updates, cnn_val_accuracy, label='CNN Val Accuracy', color='green', linestyle='--')\naxs[0, 1].plot(lstm_updates, lstm_accuracy, label='LSTM Train Accuracy', color='red')\naxs[0, 1].plot(lstm_updates, lstm_val_accuracy, label='LSTM Val Accuracy', color='red', linestyle='--')\naxs[0, 1].plot(xlstm_updates, xlstm_accuracy, label='XLSTM Train Accuracy', color='purple')\naxs[0, 1].plot(xlstm_updates, xlstm_val_accuracy, label='XLSTM Val Accuracy', color='purple', linestyle='--')\naxs[0, 1].set_title('Training & Validation Accuracy')\naxs[0, 1].set_xlabel('Incremental Update')\naxs[0, 1].set_ylabel('Regression Accuracy')\naxs[0, 1].legend(fontsize=9)\naxs[0, 1].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 3: Training Time per Incremental Update ---\naxs[1, 0].plot(hybrid_updates, hybrid_update_times, label='iTBG-Net', color='blue')\naxs[1, 0].plot(cnn_updates, cnn_update_times, label='CNN', color='green')\naxs[1, 0].plot(lstm_updates, lstm_update_times, label='LSTM', color='red')\naxs[1, 0].plot(xlstm_updates, xlstm_update_times, label='XLSTM', color='purple')\naxs[1, 0].set_title('Training Time per Incremental Update')\naxs[1, 0].set_xlabel('Incremental Update')\naxs[1, 0].set_ylabel('Time (seconds)')\naxs[1, 0].legend(fontsize=9)\naxs[1, 0].grid(True, linestyle='--', alpha=0.6)\n\n# --- Subplot 4: 48-step Forecasting Comparison ---\ntime_steps = np.arange(48)\naxs[1, 1].plot(time_steps, actual_forecast, label='Actual', color='black', marker='o')\naxs[1, 1].plot(time_steps, hybrid_forecast, label='iTBG-Net Forecast', color='blue', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, cnn_forecast, label='CNN Forecast', color='green', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, lstm_forecast, label='LSTM Forecast', color='red', linestyle='--', marker='s')\naxs[1, 1].plot(time_steps, xlstm_forecast, label='XLSTM Forecast', color='purple', linestyle='--', marker='s')\naxs[1, 1].set_title('48-step Forecasting Comparison')\naxs[1, 1].set_xlabel('Time Step')\naxs[1, 1].set_ylabel('Energy Demand (kWh)')\naxs[1, 1].legend(fontsize=9)\naxs[1, 1].grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.savefig(\"model_comparison.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:02.961682Z","iopub.execute_input":"2025-04-13T13:40:02.961878Z","iopub.status.idle":"2025-04-13T13:40:05.179651Z","shell.execute_reply.started":"2025-04-13T13:40:02.961861Z","shell.execute_reply":"2025-04-13T13:40:05.178657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport zipfile\nfrom IPython.display import FileLink, display\n\n# -----------------------------\n# Load Metrics for Each Model\n# -----------------------------\nhybrid_data = np.load(\"hybrid_metrics.npz\")\ncnn_data    = np.load(\"cnn_metrics.npz\")\nlstm_data   = np.load(\"lstm_metrics.npz\")\nxlstm_data  = np.load(\"xlstm_metrics.npz\")\n\n# For Hybrid, CNN, and incremental LSTM models, use the following keys:\n#   - incremental_update: global update number\n#   - avg_loss / avg_val_loss: training/validation loss\n#   - avg_accuracy / avg_val_accuracy: training/validation accuracy\n#   - update_time: training time per update\n#   - forecast_pred and actual: forecast predictions and ground truth\n\n# Extract arrays for the Hybrid model.\nhybrid_updates       = hybrid_data[\"incremental_update\"]\nhybrid_loss          = hybrid_data[\"avg_loss\"]\nhybrid_val_loss      = hybrid_data[\"avg_val_loss\"]\nhybrid_accuracy      = hybrid_data[\"avg_accuracy\"]\nhybrid_val_accuracy  = hybrid_data[\"avg_val_accuracy\"]\nhybrid_update_times  = hybrid_data[\"update_time\"]\nhybrid_forecast      = hybrid_data[\"forecast_pred\"]\nhybrid_actual        = hybrid_data[\"actual\"]\n\n# Extract arrays for the CNN model.\ncnn_updates       = cnn_data[\"incremental_update\"]\ncnn_loss          = cnn_data[\"avg_loss\"]\ncnn_val_loss      = cnn_data[\"avg_val_loss\"]\ncnn_accuracy      = cnn_data[\"avg_accuracy\"]\ncnn_val_accuracy  = cnn_data[\"avg_val_accuracy\"]\ncnn_update_times  = cnn_data[\"update_time\"]\ncnn_forecast      = cnn_data[\"forecast_pred\"]\ncnn_actual        = cnn_data[\"actual\"]\n\n# Extract arrays for the Incremental LSTM model.\nlstm_updates       = lstm_data[\"incremental_update\"]\nlstm_loss          = lstm_data[\"avg_loss\"]\nlstm_val_loss      = lstm_data[\"avg_val_loss\"]\nlstm_accuracy      = lstm_data[\"avg_accuracy\"]\nlstm_val_accuracy  = lstm_data[\"avg_val_accuracy\"]\nlstm_update_times  = lstm_data[\"update_time\"]\nlstm_forecast      = lstm_data[\"forecast_pred\"]\nlstm_actual        = lstm_data[\"actual\"]\n\n# Extract arrays for the xLSTM model.\nxlstm_updates       = xlstm_data[\"incremental_update\"]\nxlstm_loss          = xlstm_data[\"avg_loss\"]\nxlstm_val_loss      = xlstm_data[\"avg_val_loss\"]\nxlstm_accuracy      = xlstm_data[\"avg_accuracy\"]\nxlstm_val_accuracy  = xlstm_data[\"avg_val_accuracy\"]\nxlstm_update_times  = xlstm_data[\"update_time\"]\nxlstm_forecast      = xlstm_data[\"forecast_pred\"]\nxlstm_actual        = xlstm_data[\"actual\"]\n\n# For forecasting, assume that the actual target is identical across models.\nactual_forecast = hybrid_actual  # shape: (48,)\n\ndpi_val = 300  # DPI for saved images\n\n# -----------------------------\n# Plot 1: Training Loss vs. Incremental Update (use '*' marker)\n# -----------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(hybrid_updates, hybrid_loss, label='iTBG-Net Train Loss', color='blue', marker='*', linewidth=1)\nplt.plot(cnn_updates, cnn_loss, label='CNN (IL) Train Loss', color='green', marker='*', linewidth=1)\nplt.plot(lstm_updates, lstm_loss, label='LSTM (IL) Train Loss', color='red', marker='*', linewidth=1)\nplt.plot(xlstm_updates, xlstm_loss, label='XLSTM (IL) Train Loss', color='purple', marker='*', linewidth=1)\nplt.xlabel('Incremental Updates', fontweight='bold')\nplt.ylabel('Loss', fontweight='bold')\nplt.legend(fontsize=9, prop={'weight': 'bold'})\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"Comp_training_loss.eps\", dpi=dpi_val)\nplt.savefig(\"Comp_training_loss.pdf\", dpi=dpi_val)\nplt.savefig(\"Comp_training_loss.png\", dpi=dpi_val)\nplt.show()\n\n# -----------------------------\n# Plot 2: Validation Loss vs. Incremental Update (use '.' marker)\n# -----------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(hybrid_updates, hybrid_val_loss, label='iTBG-Net Val Loss', color='blue', linestyle='--', marker='.', linewidth=1)\nplt.plot(cnn_updates, cnn_val_loss, label='CNN (IL) Val Loss', color='green', linestyle='--', marker='.', linewidth=1)\nplt.plot(lstm_updates, lstm_val_loss, label='LSTM (IL) Val Loss', color='red', linestyle='--', marker='.', linewidth=1)\nplt.plot(xlstm_updates, xlstm_val_loss, label='XLSTM (IL) Val Loss', color='purple', linestyle='--', marker='.', linewidth=1)\nplt.xlabel('Incremental Updates', fontweight='bold')\nplt.ylabel('Loss', fontweight='bold')\nplt.legend(fontsize=9, prop={'weight': 'bold'})\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"Comp_validation_loss.eps\", dpi=dpi_val)\nplt.savefig(\"Comp_validation_loss.pdf\", dpi=dpi_val)\nplt.savefig(\"Comp_validation_loss.png\", dpi=dpi_val)\nplt.show()\n\n# -----------------------------\n# Plot 3: Training Accuracy vs. Incremental Update (use '*' marker)\n# -----------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(hybrid_updates, hybrid_accuracy, label='iTBG-Net Train Acc', color='blue', marker='*', linewidth=1)\nplt.plot(cnn_updates, cnn_accuracy, label='CNN (IL) Train Acc', color='green', marker='*', linewidth=1)\nplt.plot(lstm_updates, lstm_accuracy, label='LSTM (IL) Train Acc', color='red', marker='*', linewidth=1)\nplt.plot(xlstm_updates, xlstm_accuracy, label='XLSTM (IL) Train Acc', color='purple', marker='*', linewidth=1)\nplt.xlabel('Incremental Updates', fontweight='bold')\nplt.ylabel('Regression Accuracy', fontweight='bold')\nplt.legend(fontsize=9, prop={'weight': 'bold'})\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"Comp_training_accuracy.eps\", dpi=dpi_val)\nplt.savefig(\"Comp_training_accuracy.pdf\", dpi=dpi_val)\nplt.savefig(\"Comp_training_accuracy.png\", dpi=dpi_val)\nplt.show()\n\n# -----------------------------\n# Plot 4: Validation Accuracy vs. Incremental Update (use '.' marker)\n# -----------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(hybrid_updates, hybrid_val_accuracy, label='iTBG-Net Val Acc', color='blue', linestyle='--', marker='.', linewidth=1)\nplt.plot(cnn_updates, cnn_val_accuracy, label='CNN (IL) Val Acc', color='green', linestyle='--', marker='.', linewidth=1)\nplt.plot(lstm_updates, lstm_val_accuracy, label='LSTM (IL) Val Acc', color='red', linestyle='--', marker='.', linewidth=1)\nplt.plot(xlstm_updates, xlstm_val_accuracy, label='XLSTM (IL) Val Acc', color='purple', linestyle='--', marker='.', linewidth=1)\nplt.xlabel('Incremental Update', fontweight='bold')\nplt.ylabel('Regression Accuracy', fontweight='bold')\nplt.legend(fontsize=9, prop={'weight': 'bold'})\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"Comp_validation_accuracy.eps\", dpi=dpi_val)\nplt.savefig(\"Comp_validation_accuracy.pdf\", dpi=dpi_val)\nplt.savefig(\"Comp_validation_accuracy.png\", dpi=dpi_val)\nplt.show()\n\n# -----------------------------\n# Plot 5: Training Time per Incremental Update (with enhanced circle markers)\n# -----------------------------\nplt.figure(figsize=(6, 4))\nplt.plot(hybrid_updates, hybrid_update_times, label='iTBG-Net', marker='o', linestyle='-', markersize=5, linewidth=2,\n         color='blue', markerfacecolor='white', markeredgewidth=1.2, markeredgecolor='blue')\nplt.plot(cnn_updates, cnn_update_times, label='CNN (IL)', marker='o', linestyle='-', markersize=5, linewidth=2,\n         color='green', markerfacecolor='white', markeredgewidth=1.2, markeredgecolor='green')\nplt.plot(lstm_updates, lstm_update_times, label='LSTM (IL)', marker='o', linestyle='-', markersize=5, linewidth=2,\n         color='red', markerfacecolor='white', markeredgewidth=1.2, markeredgecolor='red')\nplt.plot(xlstm_updates, xlstm_update_times, label='XLSTM (IL)', marker='o', linestyle='-', markersize=5, linewidth=2,\n         color='purple', markerfacecolor='white', markeredgewidth=1.2, markeredgecolor='purple')\nplt.xlabel('Incremental Updates', fontweight='bold')\nplt.ylabel('Time (seconds)', fontweight='bold')\nplt.legend(fontsize=9, prop={'weight': 'bold'})\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"Comp_training_time.eps\", dpi=dpi_val)\nplt.savefig(\"Comp_training_time.pdf\", dpi=dpi_val)\nplt.savefig(\"Comp_training_time.png\", dpi=dpi_val)\nplt.show()\n\n# -----------------------------\n# Plot 6: Forecasting Comparison (48-step) with updated markers\n# -----------------------------\nplt.figure(figsize=(6, 4))\ntime_steps = np.arange(48)\n# \"Actual\" uses a star marker and firebrick color.\nplt.plot(time_steps, actual_forecast, label='Actual', color='firebrick', marker='*', linewidth=1)\n# Forecast curves use a dot marker and dashed linestyle.\nplt.plot(time_steps, hybrid_forecast, label='iTBG-Net Forecast', color='blue', linestyle='--', marker='.', linewidth=1)\nplt.plot(time_steps, cnn_forecast, label='CNN (IL) Forecast', color='green', linestyle='--', marker='.', linewidth=1)\nplt.plot(time_steps, lstm_forecast, label='LSTM (IL) Forecast', color='red', linestyle='--', marker='.', linewidth=1)\nplt.plot(time_steps, xlstm_forecast, label='XLSTM (IL) Forecast', color='purple', linestyle='--', marker='.', linewidth=1)\nplt.xlabel('Time Step', fontweight='bold')\nplt.ylabel('Energy Demand (kWh)', fontweight='bold')\nplt.legend(fontsize=9, prop={'weight': 'bold'})\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.xticks(fontweight='bold')\nplt.yticks(fontweight='bold')\nplt.tight_layout()\nplt.savefig(\"Comp_forecasting_comparison.eps\", dpi=dpi_val)\nplt.savefig(\"Comp_forecasting_comparison.pdf\", dpi=dpi_val)\nplt.savefig(\"Comp_forecasting_comparison.png\", dpi=dpi_val)\nplt.show()\n\n# -----------------------------\n# Zip all saved plot files and display a download link\n# -----------------------------\nfile_list = [\n    \"Comp_training_loss.eps\", \"Comp_training_loss.pdf\", \"Comp_training_loss.png\",\n    \"Comp_validation_loss.eps\", \"Comp_validation_loss.pdf\", \"Comp_validation_loss.png\",\n    \"Comp_training_accuracy.eps\", \"Comp_training_accuracy.pdf\", \"Comp_training_accuracy.png\",\n    \"Comp_validation_accuracy.eps\", \"Comp_validation_accuracy.pdf\", \"Comp_validation_accuracy.png\",\n    \"Comp_training_time.eps\", \"Comp_training_time.pdf\", \"Comp_training_time.png\",\n    \"Comp_forecasting_comparison.eps\", \"Comp_forecasting_comparison.pdf\", \"Comp_forecasting_comparison.png\"\n]\n\nzip_filename = \"plots.zip\"\nwith zipfile.ZipFile(zip_filename, \"w\") as zipf:\n    for filename in file_list:\n        zipf.write(filename)\n\n# Display the download link in the notebook.\ndisplay(FileLink(zip_filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:05.180655Z","iopub.execute_input":"2025-04-13T13:40:05.180914Z","iopub.status.idle":"2025-04-13T13:40:10.683372Z","shell.execute_reply.started":"2025-04-13T13:40:05.180892Z","shell.execute_reply":"2025-04-13T13:40:10.682684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install dataframe_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:10.684382Z","iopub.execute_input":"2025-04-13T13:40:10.684740Z","iopub.status.idle":"2025-04-13T13:40:17.198170Z","shell.execute_reply.started":"2025-04-13T13:40:10.684708Z","shell.execute_reply":"2025-04-13T13:40:17.197231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nest_asyncio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:17.199306Z","iopub.execute_input":"2025-04-13T13:40:17.199723Z","iopub.status.idle":"2025-04-13T13:40:20.628652Z","shell.execute_reply.started":"2025-04-13T13:40:17.199684Z","shell.execute_reply":"2025-04-13T13:40:20.627558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport zipfile\nfrom IPython.display import FileLink, display\n\n# -----------------------------\n# Load Metrics for Each Model (using absolute paths)\n# -----------------------------\nhybrid_data     = np.load(\"/kaggle/working/hybrid_metrics.npz\")\ncnn_data        = np.load(\"/kaggle/working/cnn_metrics.npz\")\nlstm_data       = np.load(\"/kaggle/working/lstm_metrics.npz\")\nxlstm_data      = np.load(\"/kaggle/working/xlstm_metrics.npz\")\n\n# For Hybrid, CNN, and incremental LSTM models, use these keys:\n#   incremental_update, avg_loss, avg_val_loss, avg_accuracy, avg_val_accuracy, update_time, forecast_pred, actual\n\n# Extract variables for the Hybrid model.\nhybrid_updates       = hybrid_data[\"incremental_update\"]\nhybrid_loss          = hybrid_data[\"avg_loss\"]\nhybrid_val_loss      = hybrid_data[\"avg_val_loss\"]\nhybrid_accuracy      = hybrid_data[\"avg_accuracy\"]\nhybrid_val_accuracy  = hybrid_data[\"avg_val_accuracy\"]\nhybrid_epoch_times   = hybrid_data[\"update_time\"]\nhybrid_forecast      = hybrid_data[\"forecast_pred\"]\nhybrid_actual        = hybrid_data[\"actual\"]\n\n# Extract variables for the CNN model.\ncnn_updates       = cnn_data[\"incremental_update\"]\ncnn_loss          = cnn_data[\"avg_loss\"]\ncnn_val_loss      = cnn_data[\"avg_val_loss\"]\ncnn_accuracy      = cnn_data[\"avg_accuracy\"]\ncnn_val_accuracy  = cnn_data[\"avg_val_accuracy\"]\ncnn_epoch_times   = cnn_data[\"update_time\"]\ncnn_forecast      = cnn_data[\"forecast_pred\"]\ncnn_actual        = cnn_data[\"actual\"]\n\n# Extract variables for the Incremental LSTM model.\nlstm_updates       = lstm_data[\"incremental_update\"]\nlstm_loss          = lstm_data[\"avg_loss\"]\nlstm_val_loss      = lstm_data[\"avg_val_loss\"]\nlstm_accuracy      = lstm_data[\"avg_accuracy\"]\nlstm_val_accuracy  = lstm_data[\"avg_val_accuracy\"]\nlstm_epoch_times   = lstm_data[\"update_time\"]\nlstm_forecast      = lstm_data[\"forecast_pred\"]\nlstm_actual        = lstm_data[\"actual\"]\n\n# Extract variables for the xLSTM model.\nxlstm_updates       = xlstm_data[\"incremental_update\"]\nxlstm_loss          = xlstm_data[\"avg_loss\"]\nxlstm_val_loss      = xlstm_data[\"avg_val_loss\"]\nxlstm_accuracy      = xlstm_data[\"avg_accuracy\"]\nxlstm_val_accuracy  = xlstm_data[\"avg_val_accuracy\"]\nxlstm_epoch_times   = xlstm_data[\"update_time\"]\nxlstm_forecast      = xlstm_data[\"forecast_pred\"]\nxlstm_actual        = xlstm_data[\"actual\"]\n\n# For forecasting, assume the actual target is identical across models.\n# (Here we use Hybrid's \"actual\".)\nactual_forecast = hybrid_actual  # shape: (48,)\n\ndpi_val = 300  # DPI for high-quality saved images\n\n# -----------------------------\n# Compute Mean Metrics for Each Model\n# -----------------------------\ndef forecast_mae(forecast, actual):\n    return np.mean(np.abs(forecast - actual))\n\nhybrid_mean_train_loss = np.mean(hybrid_loss)\nhybrid_mean_val_loss   = np.mean(hybrid_val_loss)\nhybrid_mean_train_acc  = np.mean(hybrid_accuracy)\nhybrid_mean_val_acc    = np.mean(hybrid_val_accuracy)\nhybrid_mean_time       = np.mean(hybrid_epoch_times)\nhybrid_forecast_mae    = forecast_mae(hybrid_forecast, hybrid_actual)\n\ncnn_mean_train_loss = np.mean(cnn_loss)\ncnn_mean_val_loss   = np.mean(cnn_val_loss)\ncnn_mean_train_acc  = np.mean(cnn_accuracy)\ncnn_mean_val_acc    = np.mean(cnn_val_accuracy)\ncnn_mean_time       = np.mean(cnn_epoch_times)\ncnn_forecast_mae    = forecast_mae(cnn_forecast, cnn_actual)\n\nlstm_mean_train_loss = np.mean(lstm_loss)\nlstm_mean_val_loss   = np.mean(lstm_val_loss)\nlstm_mean_train_acc  = np.mean(lstm_accuracy)\nlstm_mean_val_acc    = np.mean(lstm_val_accuracy)\nlstm_mean_time       = np.mean(lstm_epoch_times)\nlstm_forecast_mae    = forecast_mae(lstm_forecast, lstm_actual)\n\nxlstm_mean_train_loss = np.mean(xlstm_loss)\nxlstm_mean_val_loss   = np.mean(xlstm_val_loss)\nxlstm_mean_train_acc  = np.mean(xlstm_accuracy)\nxlstm_mean_val_acc    = np.mean(xlstm_val_accuracy)\nxlstm_mean_time       = np.mean(xlstm_epoch_times)\nxlstm_forecast_mae    = forecast_mae(xlstm_forecast, xlstm_actual)\n\n# -----------------------------\n# Create a Comparison DataFrame\n# -----------------------------\ndf = pd.DataFrame({\n    \"Model\": [\"Proposed iTBG-Net\", \"CNN\", \"LSTM Incremental\", \"XLSTM\"],\n    \"Train Loss\": [hybrid_mean_train_loss, cnn_mean_train_loss, lstm_mean_train_loss, xlstm_mean_train_loss],\n    \"Val Loss\": [hybrid_mean_val_loss, cnn_mean_val_loss, lstm_mean_val_loss, xlstm_mean_val_loss],\n    \"Train Acc\": [hybrid_mean_train_acc, cnn_mean_train_acc, lstm_mean_train_acc, xlstm_mean_train_acc],\n    \"Val Acc\": [hybrid_mean_val_acc, cnn_mean_val_acc, lstm_mean_val_acc, xlstm_mean_val_acc],\n    \"Update Time (s)\": [hybrid_mean_time, cnn_mean_time, lstm_mean_time, xlstm_mean_time],\n    \"Forecast MAE\": [hybrid_forecast_mae, cnn_forecast_mae, lstm_forecast_mae, xlstm_forecast_mae]\n})\n\n# -----------------------------\n# Highlight Best Values Using LaTeX Bold Formatting\n# -----------------------------\n# For metrics where lower is better (e.g., Loss, Update Time, Forecast MAE), we highlight the lowest;\n# for those where higher is better (e.g., Accuracy), we highlight the highest.\ndef format_cell(val, best, higher_is_better, decimals=6):\n    fmt_val = f\"{val:.{decimals}f}\"\n    if higher_is_better:\n        if np.isclose(val, best, atol=1e-6):\n            return r\"$\\mathbf{\" + fmt_val + \"}$\"\n    else:\n        if np.isclose(val, best, atol=1e-6):\n            return r\"$\\mathbf{\" + fmt_val + \"}$\"\n    return fmt_val\n\ndf_formatted = df.copy()\nfor col in df.columns:\n    if col == \"Model\":\n        continue\n    if col in [\"Train Acc\", \"Val Acc\"]:\n        best = df[col].max()\n        df_formatted[col] = df[col].apply(lambda x: format_cell(x, best, higher_is_better=True))\n    else:\n        best = df[col].min()\n        df_formatted[col] = df[col].apply(lambda x: format_cell(x, best, higher_is_better=False))\n\n# Create a 2D list for the table including headers.\ntable_data = [df_formatted.columns.tolist()] + df_formatted.values.tolist()\n\n# -----------------------------\n# Create a Matplotlib Table and Save as PNG and EPS\n# -----------------------------\nfig, ax = plt.subplots(figsize=(12, 3))\nax.axis('tight')\nax.axis('off')\nthe_table = ax.table(cellText=table_data, colLabels=None, loc='center', cellLoc='center')\nthe_table.auto_set_font_size(False)\nthe_table.set_fontsize(10)\nfig.tight_layout()\nplt.savefig(\"/kaggle/working/model_comparison_table.png\", dpi=300)\nplt.savefig(\"/kaggle/working/model_comparison_table.eps\", format='eps')\nplt.show()\n\n# Also print the plain DataFrame for reference.\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:20.629880Z","iopub.execute_input":"2025-04-13T13:40:20.630198Z","iopub.status.idle":"2025-04-13T13:40:21.621050Z","shell.execute_reply.started":"2025-04-13T13:40:20.630174Z","shell.execute_reply":"2025-04-13T13:40:21.619859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define model names and total training time (sum of update_time over all incremental updates)\nmodels_training_time = {\n    \"iTBG-Net\": np.sum(hybrid_update_times),\n    \"CNN (IL)\": np.sum(cnn_update_times),\n    \"LSTM (IL)\": np.sum(lstm_update_times),\n    \"XLSTM (IL)\": np.sum(xlstm_update_times),\n}\n\n# Define modern colors for each model.\nmodel_colors = {\n    \"iTBG-Net\": \"#1f77b4\",   # Blue\n    \"CNN (IL)\": \"#2ca02c\",   # Green\n    \"LSTM (IL)\": \"#d62728\",  # Red\n    \"XLSTM (IL)\": \"#9467bd\", # Purple\n}\n\n# Create a bar chart with high resolution\nplt.figure(figsize=(6, 4), dpi=600)\nbar_width = 0.5  # Set bar width\n\nbars = plt.bar(\n    list(models_training_time.keys()),\n    list(models_training_time.values()),\n    color=[model_colors[m] for m in models_training_time.keys()], \n    edgecolor='black', \n    linewidth=1.2, \n    width=bar_width, \n    alpha=0.85\n)\n\n# Add value labels on top of each bar, formatted with a thousands separator.\nmax_height = max(models_training_time.values())\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width()/2, \n        height + 0.03 * max_height,  # Extra spacing above the bar\n        f'{height:,.1f} sec', \n        ha='center', \n        va='bottom', \n        fontsize=10, \n        fontweight='bold', \n        color='black'\n    )\n\n# Adjust y-axis upper limit to leave extra space\nplt.ylim(0, max_height * 1.15)\n\n# Add axis labels and title\nplt.ylabel(\"Training Time (seconds)\", fontsize=10, fontweight=\"bold\", labelpad=10)\nplt.title(\"Training Time Comparison\", fontsize=10, fontweight=\"bold\", pad=10)\n\n# Rotate x-axis labels for better readability and set them in bold.\nplt.xticks(rotation=15, fontsize=10, fontweight=\"bold\")\n\n# Display grid only along y-axis\nplt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.6, alpha=0.7)\n\n# Save the figure in high resolution\nplt.savefig(\"Total_Training_Time_BarChart_all.eps\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Total_Training_Time_BarChart_all.png\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Total_Training_Time_BarChart_all.pdf\", dpi=600, bbox_inches=\"tight\")\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:21.622047Z","iopub.execute_input":"2025-04-13T13:40:21.622309Z","iopub.status.idle":"2025-04-13T13:40:23.585062Z","shell.execute_reply.started":"2025-04-13T13:40:21.622286Z","shell.execute_reply":"2025-04-13T13:40:23.584280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define model names and forecasted values (Non-Incremental LSTM removed)\nmodels = {\n    \"iTBG-Net\": hybrid_forecast,\n    \"CNN (IL)\": cnn_forecast,\n    \"LSTM (IL)\": lstm_forecast,\n    \"XLSTM (IL)\": xlstm_forecast,\n}\n\n# Define model colors\nmodel_colors = {\n    \"iTBG-Net\": \"#1f77b4\",    # Blue\n    \"CNN (IL)\": \"#2ca02c\",    # Green\n    \"LSTM (IL)\": \"#d62728\",   # Red\n    \"XLSTM (IL)\": \"#9467bd\",  # Purple\n}\n\n# Compute Forecasting Accuracy for each model.\n# Formula: 100 * (1 - (sum(|forecast - actual|) / sum(|actual|)))\ndef forecast_mae_percent(forecast, actual):\n    return 100 * (1 - (np.sum(np.abs(actual - forecast)) / np.sum(np.abs(actual))))\n\naccuracy_values = {}\nfor model_name, forecast in models.items():\n    accuracy = forecast_mae_percent(forecast, actual_forecast)\n    accuracy_values[model_name] = accuracy\n\n# Create a high-resolution bar chart.\nplt.figure(figsize=(6, 4), dpi=600)\nbar_width = 0.5\nbars = plt.bar(\n    list(accuracy_values.keys()),\n    list(accuracy_values.values()),\n    color=[model_colors[m] for m in accuracy_values.keys()],\n    edgecolor='black',\n    linewidth=1.2,\n    width=bar_width,\n    alpha=0.85\n)\n\n# Add value labels on top of each bar (formatted to 2 decimal places).\nmax_height = max(accuracy_values.values())\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width()/2,\n        height + 0.02 * max_height,  # Extra spacing\n        f'{height:.2f}%',\n        ha='center',\n        va='bottom',\n        fontsize=10,\n        fontweight='bold',\n        color='black'\n    )\n\nplt.ylim(0, max_height * 1.15)  # Adding 15% extra space above the tallest bar\nplt.ylabel(\"Forecasting Accuracy (%)\", fontsize=10, fontweight='bold', labelpad=10)\nplt.title(\"Accuracy Comparison of 48-Hour Forecasts\", fontsize=10, fontweight='bold', pad=10)\nplt.xticks(rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle='--', linewidth=0.6, alpha=0.7)\nplt.savefig(\"Forecasting_Accuracy_BarChart.png\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Forecasting_Accuracy_BarChart.eps\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Forecasting_Accuracy_BarChart.pdf\", dpi=600, bbox_inches=\"tight\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:23.586059Z","iopub.execute_input":"2025-04-13T13:40:23.586460Z","iopub.status.idle":"2025-04-13T13:40:23.593379Z","shell.execute_reply.started":"2025-04-13T13:40:23.586395Z","shell.execute_reply":"2025-04-13T13:40:23.592434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\n\n# ==============================================================================\n# 1. Data Loading and Splitting into Training and Forecasting Sets\n# ==============================================================================\n# Load the dataset and sort by timestamp\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\nfeatures = ['Energy_Demand']\nsequence_length = 48  \nprediction_length = 48  \n\n# Use the last prediction_length (48) records as forecasting data.\nn_total = len(data)\nn_forecast = prediction_length          # Last 48 records\nn_train = n_total - n_forecast         # Remaining records for training\n\ntraining_data = data.iloc[:n_train].copy()\nforecast_data = data.iloc[n_train:].copy()\n\nprint(\"Total samples in dataset:\", n_total)\nprint(\"Forecasting samples (last 48):\", n_forecast)\nprint(\"Training samples:\", n_train)\n\n# ==============================================================================\n# 2. Mean-Based Scaling\n# ==============================================================================\n# Compute the training mean and scale both training and forecasting data accordingly.\ntraining_mean = training_data[features].mean().values[0]\ntraining_features = training_data[features].values / training_mean\nforecast_features = forecast_data[features].values / training_mean\n\n# ==============================================================================\n# 3. Generate Overlapping Sequences from Training Data\n# ==============================================================================\n# Create sliding-window sequences and corresponding labels from training data.\ntraining_sequences = []\ntraining_labels = []\nfor i in range(len(training_features) - sequence_length - prediction_length + 1):\n    seq = training_features[i : i + sequence_length]\n    label = training_features[i + sequence_length : i + sequence_length + prediction_length]\n    training_sequences.append(seq)\n    training_labels.append(label)\n\ntraining_sequences = np.array(training_sequences)         # Shape: (N, 48, 1)\ntraining_labels = np.array(training_labels, dtype=np.float32)  # Shape: (N, 48, 1)\n\nprint(\"Total training sequences generated:\", len(training_sequences))\n\n# ==============================================================================\n# 4. Define the Forecasting (Test) Set for Comparison\n# ==============================================================================\n# Define the test sequence as the last 48 hours of training_features.\n# The corresponding ground truth (test_target) is the scaled forecast data.\ntest_sequence = training_features[-sequence_length:]  # Shape: (48, 1)\ntest_sequence = np.expand_dims(test_sequence, axis=0)   # Shape: (1, 48, 1)\n\n# Ground truth for forecasting comes from forecast_data.\ntest_target = forecast_features.flatten()               # Shape: (48,)\n\n# ==============================================================================\n# 5. Custom Objects for Keras Model Loading (if needed)\n# ==============================================================================\nclass GetItem(tf.keras.layers.Layer):\n    def __init__(self, index=slice(None, None, None), **kwargs):\n        super(GetItem, self).__init__(**kwargs)\n        self.index = index\n\n    def call(self, inputs):\n        return inputs[self.index]\n\n    def get_config(self):\n        config = super(GetItem, self).get_config()\n        config.update({\"index\": (self.index.start, self.index.stop, self.index.step)})\n        return config\n\ndef r2(y_true, y_pred):\n    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n    return 1 - SS_res/(SS_tot + tf.keras.backend.epsilon())\n\ndef clipped_mape(y_true, y_pred):\n    epsilon = 1e-3\n    y_true_clipped = tf.maximum(y_true, epsilon)\n    return tf.reduce_mean(tf.abs((y_true - y_pred)/y_true_clipped)) * 100\n\ncustom_objects = {\n    \"clipped_mape\": clipped_mape,\n    \"r2\": r2,\n    \"r2_metric\": tf.keras.metrics.MeanMetricWrapper(r2, name=\"r2\"),\n    \"GetItem\": GetItem\n}\n\n# ==============================================================================\n# 6. Model Prediction on the Forecasting Set\n# ==============================================================================\n# Assumption: These models have already been trained (or loaded) beforehand.\n# Replace the following dummy variables with your actual model instances.\n#\n# For Keras models, we use .predict(), and for the PyTorch model, we forward the test tensor.\nhybrid_pred = hybrid_model.predict(test_sequence)[0].flatten()       # Proposed iTBG-Net\ncnn_pred = cnn_model.predict(test_sequence)[0].flatten()             # CNN incremental model\nlstm_inc_pred = lstm_model.predict(test_sequence)[0].flatten()       # LSTM incremental model\n\n# For the XLSTM incremental model (in PyTorch)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodelxlstm.to(device)\ntest_tensor = torch.tensor(test_sequence, dtype=torch.float32).to(device)  # Shape: (1, 48, 1)\nmodelxlstm.eval()\nwith torch.no_grad():\n    xlstm_pred = modelxlstm(test_tensor).cpu().numpy().flatten()\n\n# ==============================================================================\n# 7. Compute Evaluation Metrics (MAE and MSE)\n# ==============================================================================\ndef compute_metrics(forecast, actual):\n    forecast = forecast.flatten()\n    actual = actual.flatten()\n    mae = np.mean(np.abs(forecast - actual))\n    mse = np.mean((forecast - actual)**2)\n    return mae, mse\n\nhybrid_mae, hybrid_mse = compute_metrics(hybrid_pred, test_target)\ncnn_mae, cnn_mse       = compute_metrics(cnn_pred, test_target)\nlstm_inc_mae, lstm_inc_mse = compute_metrics(lstm_inc_pred, test_target)\nxlstm_mae, xlstm_mse   = compute_metrics(xlstm_pred, test_target)\n\n# ==============================================================================\n# 8. Create a Comparison DataFrame and Print the Results\n# ==============================================================================\ndf = pd.DataFrame({\n    \"Model\": [\"Proposed iTBG-Net\", \"CNN (Incremental)\", \"LSTM (Incremental)\", \"XLSTM (Incremental)\"],\n    \"MAE\": [hybrid_mae, cnn_mae, lstm_inc_mae, xlstm_mae],\n    \"MSE\": [hybrid_mse, cnn_mse, lstm_inc_mse, xlstm_mse]\n})\nprint(df)\n\n# ==============================================================================\n# 9. Visualization: Bar Plots for MAE and MSE on the Forecasting Set\n# ==============================================================================\nx = np.arange(len(df[\"Model\"]))\nbar_width = 0.5\ncolors = [\"#1f77b4\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]\n\n# MAE Comparison Plot\nplt.figure(figsize=(6, 4), dpi=300)\nplt.bar(x, df[\"MAE\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"MAE Comparison on Forecasting Set\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"MAE\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"mae_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# MSE Comparison Plot\nplt.figure(figsize=(6, 4), dpi=300)\nplt.bar(x, df[\"MSE\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"MSE Comparison on Forecasting Set\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"MSE\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"mse_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:23.594315Z","iopub.execute_input":"2025-04-13T13:40:23.594585Z","iopub.status.idle":"2025-04-13T13:40:23.615699Z","shell.execute_reply.started":"2025-04-13T13:40:23.594564Z","shell.execute_reply":"2025-04-13T13:40:23.614931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\n\n# ==============================================================================\n# 1. Data Loading and Splitting (Only for Scaling Purposes)\n# ==============================================================================\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\nfeatures = ['Energy_Demand']\nsequence_length = 48  \nprediction_length = 48  \n\n# Reserve the last 48 samples for forecasting.\nn_total = len(data)\nn_forecast = prediction_length          # Last 48 rows are used for forecasting.\nn_train = n_total - n_forecast         # The remaining data is used only for computing the scaling parameter.\n\ntraining_data = data.iloc[:n_train].copy()   # Used solely to compute the mean.\nforecast_data = data.iloc[n_train:].copy()     # Ground truth for forecasting.\n\nprint(\"Total samples in dataset:\", n_total)\nprint(\"Forecasting samples (last 48):\", n_forecast)\nprint(\"Training samples (for scaling):\", n_train)\n\n# ==============================================================================\n# 2. Mean-Based Scaling\n# ==============================================================================\n# Compute the training mean and scale both training and forecasting features accordingly.\ntraining_mean = training_data[features].mean().values[0]\ntraining_features = training_data[features].values / training_mean\nforecast_features = forecast_data[features].values / training_mean\n\n# ==============================================================================\n# 3. Define Forecasting Input Sequence and Ground Truth Target\n# ==============================================================================\n# Use the last 48 hours from the scaled training features to form the forecasting input.\nforecast_sequence = training_features[-sequence_length:]   # Shape: (48, 1)\nforecast_sequence = np.expand_dims(forecast_sequence, axis=0)  # Shape: (1, 48, 1)\n\n# The ground truth target is taken from the reserved forecast_data.\ntest_target = forecast_features.flatten()  # Shape: (48,)\n\n# ==============================================================================\n# 4. Custom Objects for Keras Model Loading (if needed)\n# ==============================================================================\nclass GetItem(tf.keras.layers.Layer):\n    def __init__(self, index=slice(None, None, None), **kwargs):\n        super(GetItem, self).__init__(**kwargs)\n        self.index = index\n\n    def call(self, inputs):\n        return inputs[self.index]\n\n    def get_config(self):\n        config = super(GetItem, self).get_config()\n        config.update({\"index\": (self.index.start, self.index.stop, self.index.step)})\n        return config\n\ndef r2(y_true, y_pred):\n    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n    return 1 - SS_res/(SS_tot + tf.keras.backend.epsilon())\n\ndef clipped_mape(y_true, y_pred):\n    epsilon = 1e-3\n    y_true_clipped = tf.maximum(y_true, epsilon)\n    return tf.reduce_mean(tf.abs((y_true - y_pred) / y_true_clipped)) * 100\n\ncustom_objects = {\n    \"clipped_mape\": clipped_mape,\n    \"r2\": r2,\n    \"r2_metric\": tf.keras.metrics.MeanMetricWrapper(r2, name=\"r2\"),\n    \"GetItem\": GetItem\n}\n\n# ==============================================================================\n# 5. Model Prediction on the Forecasting Set\n# ==============================================================================\n# Assumption: The following models are already trained/loaded.\n# Replace these dummy model instances with your actual model objects.\nhybrid_pred = hybrid_model.predict(forecast_sequence)[0].flatten()       # Proposed iTBG-Net\ncnn_pred = cnn_model.predict(forecast_sequence)[0].flatten()             # CNN (Incremental)\nlstm_inc_pred = lstm_model.predict(forecast_sequence)[0].flatten()       # LSTM (Incremental)\n\n# For the XLSTM incremental model (PyTorch)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodelxlstm.to(device)\ntest_tensor = torch.tensor(forecast_sequence, dtype=torch.float32).to(device)\nmodelxlstm.eval()\nwith torch.no_grad():\n    xlstm_pred = modelxlstm(test_tensor).cpu().numpy().flatten()\n\n# ==============================================================================\n# 6. Compute Evaluation Metrics (MAE, MSE)\n# ==============================================================================\ndef compute_metrics(forecast, actual):\n    forecast = forecast.flatten()\n    actual = actual.flatten()\n    mae = np.mean(np.abs(forecast - actual))\n    mse = np.mean((forecast - actual)**2)\n    return mae, mse\n\nhybrid_mae, hybrid_mse = compute_metrics(hybrid_pred, test_target)\ncnn_mae, cnn_mse = compute_metrics(cnn_pred, test_target)\nlstm_inc_mae, lstm_inc_mse = compute_metrics(lstm_inc_pred, test_target)\nxlstm_mae, xlstm_mse = compute_metrics(xlstm_pred, test_target)\n\n# Create a DataFrame to compare the MAE and MSE of the models.\ndf_metrics = pd.DataFrame({\n    \"Model\": [\"Proposed iTBG-Net\", \"CNN (Incremental)\", \"LSTM (Incremental)\", \"XLSTM (Incremental)\"],\n    \"MAE\": [hybrid_mae, cnn_mae, lstm_inc_mae, xlstm_mae],\n    \"MSE\": [hybrid_mse, cnn_mse, lstm_inc_mse, xlstm_mse]\n})\nprint(df_metrics)\n\n# ==============================================================================\n# 7. Visualization: Bar Plots for MAE and MSE on the Forecasting Set\n# ==============================================================================\nx = np.arange(len(df_metrics[\"Model\"]))\nbar_width = 0.5\ncolors = [\"#1f77b4\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]\n\n# MAE Bar Plot\nplt.figure(figsize=(6, 4), dpi=300)\nplt.bar(x, df_metrics[\"MAE\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"MAE Comparison on Forecasting Set\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"MAE\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df_metrics[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"mae_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# MSE Bar Plot\nplt.figure(figsize=(6, 4), dpi=300)\nplt.bar(x, df_metrics[\"MSE\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"MSE Comparison on Forecasting Set\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"MSE\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df_metrics[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"mse_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\"\"\"# ==============================================================================\n# Accuracy Bar Plot for Model Comparison\n# ==============================================================================\n# Define a simple accuracy metric: percentage of predictions with error below a chosen tolerance.\ndef compute_accuracy(pred, actual, tol=0.1):\n    # Returns the accuracy (%) where each prediction is considered accurate if its absolute error is less than tol.\n    acc = np.mean(np.abs(pred - actual) < tol) * 100\n    return acc\n\nhybrid_acc = compute_accuracy(hybrid_pred, test_target)\ncnn_acc = compute_accuracy(cnn_pred, test_target)\nlstm_inc_acc = compute_accuracy(lstm_inc_pred, test_target)\nxlstm_acc = compute_accuracy(xlstm_pred, test_target)\n\ndf_accuracy = pd.DataFrame({\n    \"Model\": [\"Proposed iTBG-Net\", \"CNN (Incremental)\", \"LSTM (Incremental)\", \"XLSTM (Incremental)\"],\n    \"Accuracy (%)\": [hybrid_acc, cnn_acc, lstm_inc_acc, xlstm_acc]\n})\nprint(df_accuracy)\n\n# Accuracy Bar Plot\nplt.figure(figsize=(6, 4), dpi=300)\nplt.bar(x, df_accuracy[\"Accuracy (%)\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"Forecasting Accuracy Comparison\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Accuracy (%)\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df_accuracy[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.ylim(0, 100)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.savefig(\"accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:23.616636Z","iopub.execute_input":"2025-04-13T13:40:23.616854Z","iopub.status.idle":"2025-04-13T13:40:25.049011Z","shell.execute_reply.started":"2025-04-13T13:40:23.616837Z","shell.execute_reply":"2025-04-13T13:40:25.047835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\n\n# ==============================================================================\n# 1. Data Loading and Splitting (Only for Scaling Purposes)\n# ==============================================================================\ndata = pd.read_csv('/kaggle/input/integrated-energy-management-and-forecasting/Integrated Energy Management and Forecasting Dataset.csv')\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'])\ndata.sort_values('Timestamp', inplace=True)\n\nfeatures = ['Energy_Demand']\nsequence_length = 48  \nprediction_length = 48  \n\n# Reserve the last 48 samples for forecasting.\nn_total = len(data)\nn_forecast = prediction_length          # Last 48 rows used for forecasting.\nn_train = n_total - n_forecast         # The remaining data is used only for computing the scaling parameter.\n\ntraining_data = data.iloc[:n_train].copy()   # Used solely to compute the mean.\nforecast_data = data.iloc[n_train:].copy()     # Ground truth for forecasting.\n\nprint(\"Total samples in dataset:\", n_total)\nprint(\"Forecasting samples (last 48):\", n_forecast)\nprint(\"Training samples (for scaling):\", n_train)\n\n# ==============================================================================\n# 2. Mean-Based Scaling\n# ==============================================================================\n# Compute the training mean and scale both training and forecasting features accordingly.\ntraining_mean = training_data[features].mean().values[0]\ntraining_features = training_data[features].values / training_mean\nforecast_features = forecast_data[features].values / training_mean\n\n# ==============================================================================\n# 3. Define Forecasting Input Sequence and Ground Truth Target\n# ==============================================================================\n# Use the last 48 hours from the scaled training features to form the forecasting input.\nforecast_sequence = training_features[-sequence_length:]   # Shape: (48, 1)\nforecast_sequence = np.expand_dims(forecast_sequence, axis=0)  # Shape: (1, 48, 1)\n\n# The ground truth target is taken from the reserved forecast_data.\ntest_target = forecast_features.flatten()  # Shape: (48,)\n\n# ==============================================================================\n# 4. Custom Objects for Keras Model Loading (if needed)\n# ==============================================================================\nclass GetItem(tf.keras.layers.Layer):\n    def __init__(self, index=slice(None, None, None), **kwargs):\n        super(GetItem, self).__init__(**kwargs)\n        self.index = index\n\n    def call(self, inputs):\n        return inputs[self.index]\n\n    def get_config(self):\n        config = super(GetItem, self).get_config()\n        config.update({\"index\": (self.index.start, self.index.stop, self.index.step)})\n        return config\n\ndef r2(y_true, y_pred):\n    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n    return 1 - SS_res/(SS_tot + tf.keras.backend.epsilon())\n\ndef clipped_mape(y_true, y_pred):\n    epsilon = 1e-3\n    y_true_clipped = tf.maximum(y_true, epsilon)\n    return tf.reduce_mean(tf.abs((y_true - y_pred) / y_true_clipped)) * 100\n\ncustom_objects = {\n    \"clipped_mape\": clipped_mape,\n    \"r2\": r2,\n    \"r2_metric\": tf.keras.metrics.MeanMetricWrapper(r2, name=\"r2\"),\n    \"GetItem\": GetItem\n}\n\n# ==============================================================================\n# 5. Model Prediction on the Forecasting Set\n# ==============================================================================\n# Assumption: The following models are already trained/loaded.\n# Replace these dummy model instances with your actual model objects.\nhybrid_pred = hybrid_model.predict(forecast_sequence)[0].flatten()       # Proposed iTBG-Net\ncnn_pred = cnn_model.predict(forecast_sequence)[0].flatten()             # CNN (Incremental)\nlstm_inc_pred = lstm_model.predict(forecast_sequence)[0].flatten()       # LSTM (Incremental)\n\n# For the XLSTM incremental model (PyTorch)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodelxlstm.to(device)\ntest_tensor = torch.tensor(forecast_sequence, dtype=torch.float32).to(device)\nmodelxlstm.eval()\nwith torch.no_grad():\n    xlstm_pred = modelxlstm(test_tensor).cpu().numpy().flatten()\n\n# ==============================================================================\n# 6. Compute Evaluation Metrics (MAE, MSE)\n# ==============================================================================\ndef compute_metrics(forecast, actual):\n    forecast = forecast.flatten()\n    actual = actual.flatten()\n    mae = np.mean(np.abs(forecast - actual))\n    mse = np.mean((forecast - actual)**2)\n    return mae, mse\n\nhybrid_mae, hybrid_mse = compute_metrics(hybrid_pred, test_target)\ncnn_mae, cnn_mse = compute_metrics(cnn_pred, test_target)\nlstm_inc_mae, lstm_inc_mse = compute_metrics(lstm_inc_pred, test_target)\nxlstm_mae, xlstm_mse = compute_metrics(xlstm_pred, test_target)\n\n# Create a DataFrame to compare the MAE and MSE of the models.\ndf_metrics = pd.DataFrame({\n    \"Model\": [\"Proposed iTBG-Net\", \"CNN (Incremental)\", \"LSTM (Incremental)\", \"XLSTM (Incremental)\"],\n    \"MAE\": [hybrid_mae, cnn_mae, lstm_inc_mae, xlstm_mae],\n    \"MSE\": [hybrid_mse, cnn_mse, lstm_inc_mse, xlstm_mse]\n})\nprint(df_metrics)\n\n# ==============================================================================\n# 7. Visualization: Bar Plots for MAE and MSE on the Forecasting Set with Annotations\n# ==============================================================================\nx = np.arange(len(df_metrics[\"Model\"]))\nbar_width = 0.5\ncolors = [\"#1f77b4\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]\n\n# MAE Bar Plot\nplt.figure(figsize=(6, 4), dpi=300)\nrects = plt.bar(x, df_metrics[\"MAE\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"MAE Comparison on Forecasting Set\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"MAE\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df_metrics[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n# Annotate bars\nfor rect in rects:\n    height = rect.get_height()\n    plt.annotate(f'{height:.2f}',\n                 xy=(rect.get_x() + rect.get_width() / 2, height),\n                 xytext=(0, 3),  # Vertical offset\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom', fontsize=10)\nplt.tight_layout()\nplt.savefig(\"mae_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# MSE Bar Plot\nplt.figure(figsize=(6, 4), dpi=300)\nrects = plt.bar(x, df_metrics[\"MSE\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"MSE Comparison on Forecasting Set\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"MSE\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df_metrics[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n# Annotate bars\nfor rect in rects:\n    height = rect.get_height()\n    plt.annotate(f'{height:.2f}',\n                 xy=(rect.get_x() + rect.get_width() / 2, height),\n                 xytext=(0, 3),\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom', fontsize=10)\nplt.tight_layout()\nplt.savefig(\"mse_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# ==============================================================================\n# 8. Additional Accuracy Bar Plot for Model Comparison with Annotations\n# ==============================================================================\n# Define a simple accuracy metric: percentage of predictions with error below a chosen tolerance.\ndef compute_accuracy(pred, actual, tol=0.1):\n    # Returns the accuracy (%) where each prediction is considered accurate if its absolute error is less than tol.\n    acc = np.mean(np.abs(pred - actual) < tol) * 100\n    return acc\n\nhybrid_acc = compute_accuracy(hybrid_pred, test_target)\ncnn_acc = compute_accuracy(cnn_pred, test_target)\nlstm_inc_acc = compute_accuracy(lstm_inc_pred, test_target)\nxlstm_acc = compute_accuracy(xlstm_pred, test_target)\n\ndf_accuracy = pd.DataFrame({\n    \"Model\": [\"Proposed iTBG-Net\", \"CNN (Incremental)\", \"LSTM (Incremental)\", \"XLSTM (Incremental)\"],\n    \"Accuracy (%)\": [hybrid_acc, cnn_acc, lstm_inc_acc, xlstm_acc]\n})\nprint(df_accuracy)\n\nplt.figure(figsize=(6, 4), dpi=300)\nrects = plt.bar(x, df_accuracy[\"Accuracy (%)\"], color=colors, width=bar_width, edgecolor=\"black\")\nplt.title(\"Forecasting Accuracy Comparison\", fontsize=10, fontweight=\"bold\")\nplt.ylabel(\"Accuracy (%)\", fontsize=10, fontweight=\"bold\")\nplt.xticks(x, df_accuracy[\"Model\"], rotation=15, fontsize=10, fontweight=\"bold\")\nplt.ylim(0, 100)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n# Annotate bars\nfor rect in rects:\n    height = rect.get_height()\n    plt.annotate(f'{height:.2f}',\n                 xy=(rect.get_x() + rect.get_width() / 2, height),\n                 xytext=(0, 3),\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom', fontsize=10)\nplt.tight_layout()\nplt.savefig(\"accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:25.050133Z","iopub.execute_input":"2025-04-13T13:40:25.050506Z","iopub.status.idle":"2025-04-13T13:40:25.058229Z","shell.execute_reply.started":"2025-04-13T13:40:25.050475Z","shell.execute_reply":"2025-04-13T13:40:25.057500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After **Running this code for three times and get the values** from the training time and the accuracy, making the statistical error plot","metadata":{}},{"cell_type":"code","source":"\"\"\"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data Taken After running complete notebook for three times and noted values\naccuracy_data = {\n    \"iTBG-Net\": [91.68, 90.92, 91.68],\n    \"CNN (IL)\": [87.14, 87.14, 87.14],\n    \"LSTM (IL)\": [87.77, 52.87, 87.77],\n    \"LSTM (Non-IL)\": [50.27, 49.07, 50.27],\n    \"XLSTM (IL)\": [89.62, 90.55, 89.62],\n}\n\nmodel_colors = {\n    \"iTBG-Net\": \"#1f77b4\",\n    \"CNN (IL)\": \"#2ca02c\",\n    \"LSTM (IL)\": \"#d62728\",\n    \"LSTM (Non-IL)\": \"#ff7f0e\",\n    \"XLSTM (IL)\": \"#9467bd\",\n}\n\n# Mean & Std\nmean_accuracies = {m: np.mean(v) for m, v in accuracy_data.items()}\nstd_devs = {m: np.std(v) for m, v in accuracy_data.items()}\nmodels = list(mean_accuracies.keys())\nmeans = list(mean_accuracies.values())\nerrors = [std_devs[m] for m in models]\n\n# Plot\nplt.figure(figsize=(6, 4), dpi=600)\nbars = plt.bar(models, means, yerr=errors, capsize=5,\n               color=[model_colors[m] for m in models],\n               edgecolor='black', linewidth=1.2, width=0.55, alpha=0.85)\n\nmax_height = max([m + e for m, e in zip(means, errors)])\ntext_offset = max_height * 0.04  # More spacing above error bars\n\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    err = errors[i]\n    plt.text(bar.get_x() + bar.get_width() / 2,\n             height + err + text_offset,\n             f'{height:.2f}  {err:.2f}', ha='center', va='bottom',\n             fontsize=9, fontweight='bold', color='black')\n\nplt.ylim(0, max_height * 1.25)\nplt.ylabel(\"Forecasting Accuracy (%)\", fontsize=10, fontweight=\"bold\", labelpad=10)\n#plt.title(\"48-Hour Ahead Forecasting Accuracy Comparison\", fontsize=10, fontweight=\"bold\", pad=10)\nplt.xticks(rotation=15, fontsize=10, fontweight=\"bold\")\n\n# Only horizontal gridlines\nplt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.6, alpha=0.7)\n\nplt.tight_layout()\n\n# Save in high-res formats\nplt.savefig(\"Forecasting_Accuracy_Std.png\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Forecasting_Accuracy_Std.eps\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Forecasting_Accuracy_Std.pdf\", dpi=600, bbox_inches=\"tight\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:25.059116Z","iopub.execute_input":"2025-04-13T13:40:25.059477Z","iopub.status.idle":"2025-04-13T13:40:25.076657Z","shell.execute_reply.started":"2025-04-13T13:40:25.059446Z","shell.execute_reply":"2025-04-13T13:40:25.075854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Time after 3 time run and count the values**","metadata":{}},{"cell_type":"code","source":"\"\"\"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data Taken After running complete notebook for three times and noted values\ntraining_time_data = {\n    \"iTBG-Net\": [588.0, 636.9, 588.0],\n    \"CNN (IL)\": [484.2, 539.7, 484.2],\n    \"LSTM (IL)\": [468.5, 554.4, 468.5],\n    \"LSTM (Non-IL)\": [1022.2, 1157.6, 1022.2],\n    \"XLSTM (IL)\": [65.3, 69.4, 65.3],\n}\n\nmodel_colors = {\n    \"iTBG-Net\": \"#1f77b4\",\n    \"CNN (IL)\": \"#2ca02c\",\n    \"LSTM (IL)\": \"#d62728\",\n    \"LSTM (Non-IL)\": \"#ff7f0e\",\n    \"XLSTM (IL)\": \"#9467bd\",\n}\n\n# Mean & Std\nmean_times = {m: np.mean(v) for m, v in training_time_data.items()}\nstd_devs = {m: np.std(v) for m, v in training_time_data.items()}\nmodels = list(mean_times.keys())\nmeans = list(mean_times.values())\nerrors = [std_devs[m] for m in models]\n\n# Plot\nplt.figure(figsize=(6, 4), dpi=600)\nbars = plt.bar(models, means, yerr=errors, capsize=5,\n               color=[model_colors[m] for m in models],\n               edgecolor='black', linewidth=1.2, width=0.55, alpha=0.85)  # Slightly increased width\n\nmax_height = max([m + e for m, e in zip(means, errors)])\ntext_offset = max_height * 0.04  # Offset for vertical spacing\n\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    err = errors[i]\n    plt.text(bar.get_x() + bar.get_width() / 2,\n             height + err + text_offset,\n             f'{height:,.1f}  {err:.1f}',\n             ha='center', va='bottom',\n             fontsize=9, fontweight='bold', color='black')\n\n# Axis settings\nplt.ylim(0, max_height * 1.25)\nplt.ylabel(\"Training Time (seconds)\", fontsize=10, fontweight=\"bold\", labelpad=10)\n#plt.title(\"Training Time Comparison\", fontsize=10, fontweight=\"bold\", pad=10)\nplt.xticks(rotation=15, fontsize=10, fontweight=\"bold\")\n\n# Only horizontal gridlines\nplt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.6, alpha=0.7)\n\nplt.tight_layout()\n\n# Save in high-res formats\nplt.savefig(\"Training_Time_Std.png\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Training_Time_Std.eps\", dpi=600, bbox_inches=\"tight\")\nplt.savefig(\"Training_Time_Std.pdf\", dpi=600, bbox_inches=\"tight\")\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:25.077342Z","iopub.execute_input":"2025-04-13T13:40:25.077602Z","iopub.status.idle":"2025-04-13T13:40:25.097222Z","shell.execute_reply.started":"2025-04-13T13:40:25.077571Z","shell.execute_reply":"2025-04-13T13:40:25.096621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom IPython.display import FileLink\n\n# Define the directory and output zip path\ndirectory = \"/kaggle/working/\"\nzip_filename = os.path.join(directory, \"complete_plots.zip\")\n\n# Create the zip file\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(directory):\n        # Only zip files in the root directory\n        if root != directory:\n            continue\n        for file in files:\n            if file == \"complete_plots.zip\":\n                continue  # Skip the output zip itself\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, start=directory)\n            zipf.write(file_path, arcname)\n\n# Show download link\nprint(\"Zipping completed.\")\nFileLink(zip_filename)\ncomple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:40:25.097971Z","iopub.execute_input":"2025-04-13T13:40:25.098160Z","iopub.status.idle":"2025-04-13T13:40:25.823384Z","shell.execute_reply.started":"2025-04-13T13:40:25.098143Z","shell.execute_reply":"2025-04-13T13:40:25.822078Z"}},"outputs":[],"execution_count":null}]}